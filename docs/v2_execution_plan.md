# burn-qed v2: Competition-Focused Execution Plan

**Strategic shift:** Drop Mathlib4 as primary training source. Retrain from scratch on competition-level data (Goedel Workbook migrated to Lean 4.27, LEAN-GitHub) that matches miniF2F evaluation distribution. Reset iteration counter. Establish rigorous embedding and EBM baselines before introducing joint training. Lean Workbook (InternLM) tactic pairs deprecated — fully subsumed by Goedel 4.27 migration. NuminaMath-LEAN deferred to Phase 2. Evaluate on miniF2F-v2s (primary), v2c (stretch), and v1 (backward compat).

**Why:** LoRA r=32–64 has limited capacity. Spending it on Mathlib's 122K abstract theorems (category theory, topology, measure theory) when the eval is competition algebra/number theory is wasting parameters on out-of-distribution patterns. Workbook proofs were generated by DeepSeek-Prover-V1.5 — already in our base model's (DeepSeek-Prover-V2-7B) natural tactic vocabulary.

**Hardware:** Lambda Labs A100 40GB (~$1.29/hr). Phase M is CPU-bound (tracing, migration). Phases 0–4 need GPU for SFT, search, and joint training.

**Timeline:** ~10–12 days total, ~$40–55 GPU cost for Phases 0–4. Phase M runs ~1 week on CPU before GPU phases begin.

---

## Phase M: Lean 4.27 Migration + Data Exploration (Days M.0–M.6, ~1 week before Phase 0)

**Goal:** Migrate Goedel Workbook proofs from Lean 4.9 to 4.27, port miniF2F-v2s/v2c statements to 4.27, process LEAN-GitHub, and release community contributions. This phase produces all training data and evaluation benchmarks at the correct toolchain version.

**Why before Phase 0:** Everything downstream (tracing, SFT, search, eval) depends on having data and benchmarks compiled against Lean 4.27. Attempting to trace Goedel proofs at v4.9 against our v4.27 Pantograph will produce mass failures. Migration must happen first.

---

### Days M.0–M.2: Goedel Migration

#### Task M.1: Bulk compile attempt (Day M.0, ~4h)

```bash
# Clone and update toolchain
git clone https://github.com/Goedel-LM/Lean-workbook-proofs
cd Lean-workbook-proofs
echo "leanprover/lean4:4.27.0" > lean-toolchain
# Update lakefile.lean Mathlib dependency → 4.27-compatible tag
lake update
lake build 2>&1 | tee migration_log_phase1.txt

# Count survivors
grep -c "✓" migration_log_phase1.txt   # compiled
grep -c "✗" migration_log_phase1.txt   # failed
```

Log: compiled/total, error categories. This gives the raw survival rate before any fixes.

#### Task M.2: Automated fixes (Day M.1, ~4h)

```bash
# Parse compilation errors, apply known renames
python python/migration/auto_fix.py \
    --errors migration_log_phase1.txt \
    --apply-renames sorted→pairwise \
    --fix-nat-instances \
    --output migration_fixes.json

# Rebuild
lake build 2>&1 | tee migration_log_phase2.txt
```

Known 4.27 breaking changes for competition math:
- `sorted` → `pairwise` renames (unlikely in competition math but check)
- `Nat.sqrt` definitional reduction changes (possible in number theory)
- General Mathlib theorem name shuffles (main risk — handle with regex)

#### Task M.3: Manual triage (Day M.2, ~6h)

For remaining failures, categorize:
- Mathlib API rename → regex fix
- Definitional change → manual proof patch
- Fundamental incompatibility → drop from dataset

**Target: ≥95% survival rate (≥28,270 of 29,759).** If below 90%, reassess timeline — LEAN-GitHub supplements the gap but we still want Goedel as the primary traced source.

Document all changes for community PR.

### Day M.3: Port miniF2F-v2s/v2c to 4.27

#### Task M.4: Port evaluation benchmarks (~4h)

```bash
# Extract formal_statement fields from miniF2F_v2s.jsonl and miniF2F_v2c.jsonl
python python/migration/port_minif2f_v2.py \
    --input data/benchmarks/minif2f_v2/miniF2F_v2s.jsonl \
    --output data/benchmarks/minif2f_v2s_test_427.json \
    --lean-version 4.27.0

# Same migration pass as Goedel (most statements use foundational tactics)
# Verify all 488 compile
lean --run data/benchmarks/minif2f_v2s_test_427.json  # should report 488/488 ok

# Also port v2c
python python/migration/port_minif2f_v2.py \
    --input data/benchmarks/minif2f_v2/miniF2F_v2c.jsonl \
    --output data/benchmarks/minif2f_v2c_test_427.json \
    --lean-version 4.27.0
```

Keep v1 eval too for backward compatibility with our iteration history.

### Days M.4–M.5: Trace + Process Data

#### Task M.5: LeanDojo trace on compiled Goedel 4.27 proofs (Day M.4, ~10-15h CPU)

Parallelized LeanDojo tracing on all surviving Goedel proofs from Days M.0–M.2. Key parameters:
- `NUM_WORKERS = min(16, int(cpu_count() * 0.75))` — cap to avoid OOM (Gotcha 15)
- `PER_THEOREM_TIMEOUT = 120s` — kill slow theorems, prevent zombies (Gotcha 10)
- `CHUNK_SIZE = 500` — write checkpoints and kill zombie Lean processes at each boundary
- Sorry/admit/cheat filter: reject entire theorem if any tactic contains banned keyword (Gotcha 12)
- Use `signal.alarm()` for per-theorem timeouts, `cleanup_lean_processes()` via psutil

Script: `python/migration/parallel_trace.py` (adapted from v1 tracing pipeline)

Output: `data/traced/goedel_427_pairs.jsonl` — ~110-140K tactic pairs.

**Critical: check error rate.** If > 20% tracing failures on the 4.27-compiled proofs, investigate — may indicate Mathlib version mismatch between lake build and LeanDojo's Lean REPL.

### Day M.5: Data Exploration (after tracing, before merge)

The traced Goedel pairs are now in `data/traced/goedel_427_pairs.jsonl`. Before merging with LEAN-GitHub and committing to the SFT dataset, explore the data to catch problems that would silently degrade training and to set data-driven hyperparameters for Phase 1.

Script: `python/migration/explore_traced.py`

#### Task M.6: Compilation & integrity sweep (~1h)

Before looking at tokens or depths, verify that the migration actually produced valid Lean 4.27 code. The `lake build` pass (Day M.2) checks compilation, but tracing can introduce additional failure modes — elaboration errors that compile but produce garbage states, or proofs that silently smuggle in sorry-like axioms.

```python
"""explore_traced.py — Part 1: Compilation & integrity sweep."""

import json
from collections import Counter

pairs = [json.loads(l) for l in open("data/traced/goedel_427_pairs.jsonl")]
theorems = {}
for p in pairs:
    theorems.setdefault(p["theorem"], []).append(p)

# 1. Sorry/admit/cheat/sorryAx contamination
# The tracer should have filtered these, but verify independently
BANNED = ["sorry", "admit", "cheat", "sorryAx"]
contaminated_theorems = set()
contaminated_pairs = 0
for p in pairs:
    if any(bad in p["tactic"] for bad in BANNED):
        contaminated_theorems.add(p["theorem"])
        contaminated_pairs += 1
    # Also check if sorry appears in the STATE (indicates upstream sorry)
    if "sorry" in p["state"] or "sorryAx" in p["state"]:
        contaminated_theorems.add(p["theorem"])
        contaminated_pairs += 1

print(f"Contaminated theorems: {len(contaminated_theorems)} "
      f"({len(contaminated_theorems)/len(theorems)*100:.2f}%)")
print(f"Contaminated pairs: {contaminated_pairs}")
assert contaminated_pairs == 0, "ABORT: sorry/admit found — re-run filter"

# 2. Noncomputable / axiom usage
# Classical logic (propext, funext, choice) is fine for competition math.
# But check for unexpected axioms that could indicate proof cheating.
SUSPICIOUS_AXIOMS = ["sorryAx", "Decidable.decide"]  # not sorry but suspicious
axiom_mentions = Counter()
for p in pairs:
    for ax in SUSPICIOUS_AXIOMS:
        if ax in p["tactic"] or ax in p["state"]:
            axiom_mentions[ax] += 1
if axiom_mentions:
    print(f"Suspicious axiom usage: {dict(axiom_mentions)}")
    # Not necessarily abort — but flag for review

# 3. Empty / malformed states
malformed = 0
for p in pairs:
    if not p["state"].strip():
        malformed += 1
    elif "⊢" not in p["state"]:
        malformed += 1  # Missing turnstile — not a valid goal state
    elif p["state"].startswith("no goals"):
        malformed += 1  # Shouldn't appear in state_before
print(f"Malformed states: {malformed} ({malformed/len(pairs)*100:.2f}%)")

# 4. Definitive count
clean_theorems = len(theorems) - len(contaminated_theorems)
clean_pairs = len(pairs) - contaminated_pairs - malformed
print(f"\n=== CLEAN POOL ===")
print(f"Theorems: {clean_theorems} / {len(theorems)}")
print(f"Tactic pairs: {clean_pairs} / {len(pairs)}")
```

**Actionable output:** A definitive "Clean Pool" count of 100% verified, compiling, 4.27-compliant theorems. Everything else is dropped before merge. If clean pool < 90% of traced output, go back to Day M.2 for another fix pass.

#### Task M.7: Token geometry — VRAM & masking survival (~1h)

This directly determines `--max-length` for Day 3 SFT training and `--max-length` for the `EmbeddingExtractor`. Get it wrong and you either truncate training data silently or waste VRAM on padding.

```python
"""explore_traced.py — Part 2: Token geometry."""

from transformers import AutoTokenizer
import numpy as np

tok = AutoTokenizer.from_pretrained("deepseek-ai/DeepSeek-Prover-V2-7B")

state_lengths = []
tactic_lengths = []
full_lengths = []  # state + tactic as formatted SFT example

for p in pairs:
    state_toks = len(tok.encode(p["state"]))
    tactic_toks = len(tok.encode(p["tactic"]))
    state_lengths.append(state_toks)
    tactic_lengths.append(tactic_toks)

    # Full SFT example as it will be seen by the model
    formatted = (
        f"Complete the following Lean 4 code:\n\n"
        f"```lean4\n/- tactic state:\n{p['state']}\n-/\n```\n"
        f"{p['tactic']}"
    )
    full_lengths.append(len(tok.encode(formatted)))

state_lengths = np.array(state_lengths)
tactic_lengths = np.array(tactic_lengths)
full_lengths = np.array(full_lengths)

print("=== STATE (proof state) token lengths ===")
for pct in [50, 75, 90, 95, 99]:
    print(f"  p{pct}: {np.percentile(state_lengths, pct):.0f}")
print(f"  max: {state_lengths.max()}")

print("\n=== TACTIC (completion target) token lengths ===")
for pct in [50, 75, 90, 95, 99]:
    print(f"  p{pct}: {np.percentile(tactic_lengths, pct):.0f}")
print(f"  max: {tactic_lengths.max()}")

print("\n=== FULL SFT EXAMPLE token lengths ===")
for pct in [50, 75, 90, 95, 99]:
    print(f"  p{pct}: {np.percentile(full_lengths, pct):.0f}")
print(f"  max: {full_lengths.max()}")

# State:Tactic ratio — how much of the sequence is "wasted" on the prompt
ratios = state_lengths / np.maximum(tactic_lengths, 1)
print(f"\n=== STATE:TACTIC ratio ===")
print(f"  median: {np.median(ratios):.1f}x")
print(f"  mean: {np.mean(ratios):.1f}x")
print(f"  p95: {np.percentile(ratios, 95):.1f}x")

# Truncation analysis at candidate max_length values
print("\n=== TRUNCATION ANALYSIS ===")
for max_len in [1024, 1536, 2048, 3072]:
    truncated = np.sum(full_lengths > max_len)
    # Worse: how many lose the TACTIC entirely?
    # If state alone exceeds max_len, the tactic is fully truncated
    prompt_overhead = 45  # "Complete the following..." wrapper tokens
    tactic_lost = np.sum(state_lengths + prompt_overhead >= max_len)
    print(f"  max_length={max_len}: "
          f"{truncated} truncated ({truncated/len(full_lengths)*100:.1f}%), "
          f"{tactic_lost} lose tactic entirely ({tactic_lost/len(full_lengths)*100:.2f}%)")

# === DECISION OUTPUT ===
# Pick max_length where < 2% of examples are truncated
for candidate in [1024, 1536, 2048, 3072]:
    if np.sum(full_lengths > candidate) / len(full_lengths) < 0.02:
        print(f"\n>>> RECOMMENDED SFT max_length: {candidate}")
        break

# Embedding extractor only sees raw state (no wrapper)
for candidate in [256, 384, 512, 768, 1024]:
    if np.sum(state_lengths > candidate) / len(state_lengths) < 0.02:
        print(f">>> RECOMMENDED Encode max_length: {candidate}")
        break
```

**Actionable output:** Exact, data-driven values for:
- `--max-length` in the SFT training loop (Day 3, Task 1.1) — replaces the assumed 2048
- `--max-length` in the `EmbeddingExtractor` (Day 5, Task 2.1) — replaces the assumed 512
- If p95 state length is under 1024, you can potentially double batch size and halve GPU time

Store results in `data/traced/token_geometry.json` — referenced by Phase 1 training scripts.

#### Task M.8: Proof structure & tactic vocabulary (~1h)

Understand the shape of the data for contrastive pool sizing and stylistic diversity assessment.

```python
"""explore_traced.py — Part 3: Proof structure & tactic vocabulary."""

from collections import Counter
import re

# === 1. Proof length distribution (tactics per theorem) ===
theorem_depths = {}
theorem_pair_counts = {}
for p in pairs:
    t = p["theorem"]
    theorem_pair_counts[t] = theorem_pair_counts.get(t, 0) + 1
    theorem_depths[t] = max(theorem_depths.get(t, 0), p.get("depth", 0))

lengths = list(theorem_pair_counts.values())
depths = list(theorem_depths.values())

print("=== PROOF LENGTH (tactic count per theorem) ===")
for pct in [25, 50, 75, 90, 95]:
    print(f"  p{pct}: {np.percentile(lengths, pct):.0f}")
print(f"  mean: {np.mean(lengths):.1f}")

# Contrastive pool sizing
depth_counts = Counter()
for d in depths:
    if d >= 5: depth_counts["5+"] += 1
    elif d >= 3: depth_counts["3-4"] += 1
    elif d >= 1: depth_counts["1-2"] += 1
    else: depth_counts["0"] += 1

print(f"\n=== DEPTH DISTRIBUTION (max depth per theorem) ===")
for bucket in ["0", "1-2", "3-4", "5+"]:
    count = depth_counts[bucket]
    print(f"  depth {bucket}: {count} theorems ({count/len(theorems)*100:.1f}%)")

contrastive_eligible = sum(1 for d in depths if d >= 3)
print(f"\n  Contrastive pool (depth ≥ 3): {contrastive_eligible} theorems")
if contrastive_eligible < 3000:
    print("  WARNING: Contrastive pool may be too small for 2K-theorem deep search.")
    print("  Consider lowering depth threshold to 2, or using more search budget per theorem.")

# === 2. Tactic vocabulary ===
tactic_counts = Counter()
for p in pairs:
    # Extract the first token/command of the tactic
    tactic_text = p["tactic"].strip()
    # Get the tactic name (first word, before any arguments)
    tactic_name = re.split(r'[\s\[\(]', tactic_text)[0]
    tactic_counts[tactic_name] += 1

print(f"\n=== TOP 30 TACTICS ===")
total = sum(tactic_counts.values())
cumulative = 0
for tactic, count in tactic_counts.most_common(30):
    cumulative += count
    print(f"  {tactic:25s} {count:6d} ({count/total*100:5.1f}%)  "
          f"cumul: {cumulative/total*100:.1f}%")

# How concentrated is the vocabulary?
top5_share = sum(c for _, c in tactic_counts.most_common(5)) / total
top10_share = sum(c for _, c in tactic_counts.most_common(10)) / total
print(f"\n  Top 5 tactics cover: {top5_share*100:.1f}% of all pairs")
print(f"  Top 10 tactics cover: {top10_share*100:.1f}% of all pairs")
print(f"  Unique tactic names: {len(tactic_counts)}")

# === 3. Import surface area ===
# Parse from the full proofs (not tactic pairs) if available
import_counts = Counter()
try:
    import glob
    for f in glob.glob("data/lean/workbook/goedel_proofs_427/**/*.lean", recursive=True):
        for line in open(f):
            if line.strip().startswith("import "):
                module = line.strip().split()[1]
                import_counts[module] += 1

    print(f"\n=== TOP 20 IMPORTS ===")
    for imp, count in import_counts.most_common(20):
        print(f"  {imp:45s} {count:5d}")
    print(f"  Unique import paths: {len(import_counts)}")
except Exception as e:
    print(f"\n  Import analysis skipped (full proofs not in .lean format): {e}")
    print(f"  Run on data/lean/workbook/goedel_proofs_427/ if available.")
```

**Actionable outputs:**
- **Contrastive pool size.** If depth ≥ 3 theorems < 3,000, lower the threshold to 2 in Task M.10 merge, or accept a smaller pool and reduce the 2K-theorem search target in Day 4.
- **Tactic concentration.** If top 5 tactics cover > 70% of pairs, the LEAN-GitHub merge is especially important for diversity. If Goedel is already diverse, LEAN-GitHub's value is primarily volume.
- **Import surface area.** Narrow footprint (Mathlib.Tactic, Mathlib.Data.Nat/Int) confirms NuminaMath migration (Phase 2) will face similar manageable challenges.

Store results in `data/traced/exploration_report.json`.

#### Task M.8 deliverable (exploration)

```
data/traced/
├── goedel_427_pairs.jsonl      # From Task M.5
├── token_geometry.json          # p50/p95/p99 for states, tactics, full examples
├── exploration_report.json      # Depths, tactic vocab, import surface, clean pool count
└── exploration_plots/           # Optional: histograms for token lengths, depth distribution
```

**Decision gates from exploration:**
- `--max-length` for SFT training → set by token geometry (replaces assumed 2048)
- `--max-length` for EmbeddingExtractor → set by token geometry (replaces assumed 512)
- Contrastive pool threshold → set by depth distribution (may adjust from 3)
- LEAN-GitHub tactic subsampling rates → informed by Goedel tactic concentration

### Day M.5 (continued): Process LEAN-GitHub + Merge

#### Task M.9: Download + filter LEAN-GitHub (~2h)

```bash
huggingface-cli download internlm/Lean-Github --local-dir data/lean/lean_github
```

```python
"""filter_lean_github.py — Quality filter + format convert for LEAN-GitHub."""

from datasets import load_dataset
import random

ds = load_dataset("parquet", data_dir="data/lean/lean_github")

def should_include(row):
    state = row["state_before"]
    tactic = row["tactic"]
    
    # Skip trivially long states (likely generated / expanded)
    if len(state) > 4096:
        return False
    
    # Skip trivial single-token tactics (for diversity)
    # But keep them at 10% rate for calibration
    if tactic.strip() in {"rfl", "simp", "trivial", "exact?", "decide"}:
        return random.random() < 0.1
    
    # Skip if state is "no goals"
    if "no goals" in state:
        return False
    
    # Sorry/admit/cheat filter
    if any(bad in tactic for bad in ["sorry", "admit", "cheat"]):
        return False
    
    return True

filtered = ds.filter(should_include)
# Convert to our format with source prefix for dedup
for row in filtered:
    pairs.append({
        "state": row["state_before"],
        "tactic": row["tactic"],
        "theorem": f"github::{row['full_name']}",
        "source": "lean_github",
    })
```

Expected yield: ~100-150K usable tactic pairs. These are pre-traced and version-agnostic for SFT.

#### Task M.10: Merge into unified SFT dataset (~1h)

Combine Goedel 4.27 clean pool (from Task M.6) + LEAN-GitHub filtered pairs (from Task M.9). Deduplicate by source-prefixed theorem name. Apply sorry/admit filter. Split 95/5 by theorem.

**Use exploration results to set parameters:**
- Drop Goedel pairs flagged in integrity sweep (Task M.6)
- Contrastive depth threshold from exploration (Task M.8) — default 3, lower to 2 if pool < 3K
- LEAN-GitHub trivial tactic subsampling rate — increase if Goedel tactic vocab is already diverse

Output:
```
data/sft/
├── train.jsonl               # ~210-350K pairs from Goedel + LEAN-GitHub
├── val.jsonl                 # 5% held out, stratified by source
├── contrastive_pool.json     # Theorems with depth ≥ 3 (Goedel only — LEAN-GitHub lacks depth info)
└── stats.json                # Counts, source breakdown
```

### Day M.6: Community Release

#### Task M.11: Release migrated datasets

- `{org}/Lean-workbook-proofs-4.27` on HuggingFace — Parquet (full proofs + tactic pairs), lean-toolchain, Mathlib commit, migration changelog, dropped theorems list
- `{org}/miniF2F-v2-lean427` on GitHub — ported v2s/v2c statements + verification script
- Open PR upstream to Goedel-LM

### Phase M deliverable

```
data/
├── lean/
│   ├── workbook/goedel_proofs_427/  # Migrated Goedel proofs (Lean 4.27)
│   └── lean_github/                  # Raw LEAN-GitHub download
├── benchmarks/
│   ├── minif2f_v2s_test_427.json    # Ported miniF2F-v2s (488 problems)
│   └── minif2f_v2c_test_427.json    # Ported miniF2F-v2c (488 problems)
├── traced/
│   ├── goedel_427_pairs.jsonl       # ~110-140K tactic pairs from Goedel
│   ├── lean_github_pairs.jsonl      # ~100-150K pairs from LEAN-GitHub
│   ├── token_geometry.json          # Data-driven max_length for SFT and EmbeddingExtractor
│   ├── exploration_report.json      # Depths, tactic vocab, imports, clean pool
│   └── exploration_plots/           # Histograms (token lengths, depth dist, tactic freq)
└── sft/
    ├── train.jsonl                   # Merged, ~210-350K pairs
    ├── val.jsonl                     # 5% held out
    ├── contrastive_pool.json         # Theorems with depth ≥ 3
    └── stats.json
```

---

## Phase 0: Environment Setup + Data Pipeline (Days 1–3)

**Goal:** Validate Phase M outputs, verify tracing quality, archive old iterations. Phase M has already produced the unified SFT dataset (~210-350K pairs) and evaluation benchmarks at Lean 4.27.

**Note:** PyTorch GoalConditionedEnergyHead port (Task 1.0) moved to Phase 1 — it doesn't block any Phase 0 data work, and grouping it with SFT training keeps all model work together. Both the iter_0 decoupled baseline AND the iter_1 joint run must use the identical EBM architecture. The burn-rs EBM pipeline is deprecated; all EBM work going forward is PyTorch only.

**Prerequisite:** Phase M complete — Goedel proofs migrated to 4.27, LEAN-GitHub filtered, miniF2F-v2s/v2c ported, unified SFT dataset produced.

---

### Day 1: Validate Phase M Outputs + Archive

#### Task 0.1: Validate Phase M outputs (1h)

Phase M has resolved all version compatibility issues. Verify the outputs:

```bash
# Verify Goedel 4.27 proofs compiled
wc -l data/traced/goedel_427_pairs.jsonl    # Target: ≥ 110K pairs

# Verify LEAN-GitHub filtered
wc -l data/traced/lean_github_pairs.jsonl    # Target: ≥ 100K pairs

# Verify unified SFT dataset
wc -l data/sft/train.jsonl       # Target: ≥ 200K pairs
wc -l data/sft/val.jsonl              # ~5% of total

# Verify miniF2F-v2s/v2c benchmarks compile on 4.27
python python/eval/verify_minif2f.py \
    --v2s data/benchmarks/minif2f_v2s_test_427.json \
    --v2c data/benchmarks/minif2f_v2c_test_427.json
# Should report 488/488 ok for each

# Verify Phase M exploration outputs exist and are reasonable
python -c "
import json
geo = json.load(open('data/traced/token_geometry.json'))
print(f'SFT max_length recommendation: {geo[\"recommended_sft_max_length\"]}')
print(f'Encode max_length recommendation: {geo[\"recommended_encode_max_length\"]}')
report = json.load(open('data/traced/exploration_report.json'))
print(f'Clean pool: {report[\"clean_theorems\"]} theorems')
print(f'Contrastive eligible (depth >= 3): {report[\"contrastive_eligible\"]} theorems')
print(f'Top 5 tactic concentration: {report[\"top5_tactic_share\"]*100:.1f}%')
"

# Check source breakdown
python -c "
import json
data = [json.loads(l) for l in open('data/sft/train.jsonl')]
from collections import Counter
print(Counter(d.get('source','unknown') for d in data))
"
```

**ABORT if:** SFT dataset < 150K pairs, either source missing, or miniF2F benchmarks have compile failures. Go back to Phase M.

#### Task 0.2: Archive old iterations (1h)

```bash
mkdir -p data/archive/v1
mv data/models/merged/iter_* data/archive/v1/
mv data/checkpoints/ebm/iter_* data/archive/v1/
mv data/trajectories/*.parquet data/archive/v1/
mv docs/iter_* data/archive/v1/
mv docs/ebm_perf.md data/archive/v1/

# Create v2 directory structure
mkdir -p data/{lean/workbook,lean/numinamath,traced,sft}
mkdir -p data/{models/merged,checkpoints/lora,checkpoints/ebm}
mkdir -p data/{trajectories,embeddings,evals}
```

#### Task 0.3: Verify dataset inventory (0.5h)

Downloads completed in Phase M. Verify files are present:

```bash
# Phase M outputs (should already exist)
ls -la data/lean/workbook/goedel_proofs_427/    # Migrated Goedel proofs
ls -la data/lean/lean_github/                    # Raw LEAN-GitHub
ls -la data/sft/train.jsonl     # Unified SFT dataset
ls -la data/benchmarks/minif2f_v2s_test_427.json       # miniF2F-v2s at 4.27
ls -la data/benchmarks/minif2f_v2c_test_427.json       # miniF2F-v2c at 4.27

# NuminaMath (downloaded in Phase M but deferred to Phase 2 for processing)
ls -la data/lean/numinamath/                     # Raw download, not yet processed
```

Inventory:
```
Dataset                    Theorems      Tactic pairs (est.)   Status
──────────────────────────────────────────────────────────────────────────────────
Goedel Workbook (4.27)     ~28K+         ~110-140K             Phase M traced
LEAN-GitHub                28,597        ~100-150K             Phase M filtered
NuminaMath-LEAN            104,000       ~50-80K (est.)        Phase 2 (deferred)
──────────────────────────────────────────────────────────────────────────────────
Total available:                         ~210-350K pairs (Phase 1 ready)
```

### Day 2: Data Validation + Quality Checks

**Note:** LeanDojo tracing was completed in Phase M. Day 2 validates the traced data and prepares final SFT formatting.

#### Task 0.4: Validate traced Goedel pairs (2h)

Phase M produced `data/traced/goedel_427_pairs.jsonl`. Validate quality:

```python
"""validate_traced.py — Quality checks on Phase M traced data."""

import json

pairs = [json.loads(l) for l in open("data/traced/goedel_427_pairs.jsonl")]

# Check for sorry/admit contamination (should be 0 after Phase M filtering)
sorry_count = sum(1 for p in pairs if any(bad in p["tactic"] for bad in ["sorry", "admit", "cheat"]))
assert sorry_count == 0, f"ABORT: {sorry_count} sorry/admit pairs found"

# Check state format matches DeepSeek-native prompt expectations
for p in pairs[:100]:
    assert "⊢" in p["state"], f"Missing turnstile in state: {p['state'][:80]}"
    assert len(p["tactic"]) > 0, f"Empty tactic for theorem {p['theorem']}"
    
# Depth distribution (needed for contrastive pool)
from collections import Counter
depth_counts = Counter(p.get("depth", 0) for p in pairs)
deep_pairs = sum(v for k, v in depth_counts.items() if k >= 3)
print(f"Total pairs: {len(pairs)}")
print(f"Depth distribution: {sorted(depth_counts.items())}")
print(f"Depth ≥ 3 pairs: {deep_pairs} ({deep_pairs/len(pairs)*100:.1f}%)")
```

#### Task 0.5: NuminaMath-LEAN — DEFERRED TO PHASE 2

**Deferred.** Goedel + LEAN-GitHub already provide ~210-350K tactic pairs. NuminaMath processing will proceed in Phase 2 after Goedel migration is validated. Same parallelized pipeline when ready: filter to `formal_proof != ""` and `ground_truth_type != "with_sorry"`, migrate from v4.15 to v4.27, trace.

#### Task 0.6: Final SFT formatting and split verification (1h)

The unified dataset was merged in Phase M (Task M.10). Verify the formatting matches the Rust policy crate:

```python
# Verify format matches docs/data_format_spec.md exactly
all_pairs = [json.loads(l) for l in open("data/sft/train.jsonl")]

# Spot-check DeepSeek-native prompt format
for pair in all_pairs[:10]:
    text = pair["text"]
    assert text.startswith("Complete the following Lean 4 code:\n\n```lean4\n/- tactic state:\n"), \
        f"Bad format prefix: {text[:80]}"
    assert "\n-/\n```\n" in text, f"Missing closing fence: {text[:200]}"

# Verify source-prefixed theorem names for dedup
sources = Counter(p.get("source", "unknown") for p in all_pairs)
print(f"Source breakdown: {sources}")
# Expected: goedel_workbook ~110-140K, lean_github ~100-150K

# Verify theorem-level split (Gotcha #11)
train_data = [json.loads(l) for l in open("data/sft/train.jsonl")]
val_data = [json.loads(l) for l in open("data/sft/val.jsonl")]
train_theorems = set(p["theorem"] for p in train_data)
val_theorems = set(p["theorem"] for p in val_data)
leak = train_theorems & val_theorems
assert len(leak) == 0, f"ABORT: {len(leak)} theorems leaked between train/val"

# Separate pool for contrastive training (need depth ≥ 3, Goedel only)
contrastive = json.load(open("data/sft/contrastive_pool.json"))
print(f"Contrastive pool: {len(contrastive)} theorems with depth ≥ 3")
```

**Depth filter rationale:** Proofs with depth < 3 (single `omega`, `norm_num`, `simp` closers) have no meaningful search tree. They're fine for teaching the LLM tactics but useless for EBM training because there's nothing to discriminate. LEAN-GitHub pairs lack depth info, so contrastive pool comes from Goedel only.

#### Day 2 deliverable

Validated Phase M outputs, ready for SFT training:
```
data/sft/
├── train.jsonl               # ~210-350K pairs from Goedel + LEAN-GitHub (validated)
├── val.jsonl                 # 5% held out, theorem-level split verified
├── contrastive_pool.json     # Theorems with depth ≥ 3 (Goedel only)
└── stats.json                # Counts, depth distribution, source breakdown
```

---

## Phase 1: Iter 0 SFT + EBM Baseline (Days 3–4)

**Goal:** Port GoalConditionedEnergyHead to PyTorch, train LoRA on competition data. Full dataset (~210-350K pairs) is available from Phase M — no seed-then-retrain strategy needed.

---

### Day 3: PyTorch EBM Port + SFT Training

#### Task 1.0: Port GoalConditionedEnergyHead to PyTorch (3h)

File: `python/joint/ebm_head.py`

Both iter_0 and iter_1 use this exact architecture. Must be done before any EBM work in Phase 2.

```python
class GoalConditionedEnergyHead(nn.Module):
    """
    E(state, goal) — used for BOTH decoupled (iter_0) and joint (iter_1).
    Input: [z_state; z_goal; z_state ⊙ z_goal] → 12288
    Architecture: 12288 → 2048 → 1024 → 512 → 1
    Spectral norm on all layers. SiLU activations. Dropout 0.15.
    Learnable log_temperature clamped [0.1, 10.0].
    First layer: weight *= 0.1 (Gotcha 2).
    """
```

Also implement a thin encode() wrapper for extracting embeddings from the base model:

```python
class EmbeddingExtractor:
    """Standalone embedding extraction (no LoRA dependency).
    Used in both decoupled training (frozen encoder) and joint training.
    """
    def __init__(self, model, tokenizer):
        self.model = model
        self.tokenizer = tokenizer
        self.tokenizer.padding_side = "right"  # Gotcha 4

    def encode(self, texts, max_length=512):
        inputs = self.tokenizer(texts, return_tensors="pt",
                                padding=True, truncation=True,
                                max_length=max_length)
        outputs = self.model(**inputs, output_hidden_states=True)
        hidden = outputs.hidden_states[-1]
        seq_lengths = inputs["attention_mask"].sum(dim=1) - 1
        return hidden[torch.arange(hidden.size(0)), seq_lengths]
```

Unit tests:
- GoalConditionedEnergyHead: random input → output shape [batch, 1], spectral norms ≈ 1.0
- Verify first layer weight magnitude is ~0.1x other layers after init
- Verify temperature parameter exists and forward() divides by exp(log_temp)
- Verify EmbeddingExtractor grabs correct token (not pad) with padding_side="right"

#### Task 1.1: Train iter_0 LoRA on full dataset (8–10h GPU)

**CRITICAL: Completion-only loss masking.** Proof states are 500+ tokens. Tactics are 10–50 tokens. Without masking, 90% of gradient updates teach the LoRA to echo the environment state — a complete waste of its limited r=32 capacity. The model must ONLY compute loss on the tactic tokens after the closing code fence.

**Format:** DeepSeek-native prompt with tactic state as Lean comment (see `docs/data_format_spec.md`). No special tokens needed — uses the model's native instruction prefix and Lean comment syntax.

```python
from trl import SFTTrainer, DataCollatorForCompletionOnlyLM

# No special tokens needed — DeepSeek-native format uses standard Lean
# comment syntax and code fences which are already in the tokenizer vocab.

# === Completion-only collator — loss only on tactic tokens ===
# The closing ``` appears twice: opening (```lean4) and closing (```).
# DataCollatorForCompletionOnlyLM finds the LAST occurrence of the template,
# which correctly matches the closing fence (opening is ```lean4\n, not ```\n).
response_template = "```\n"
collator = DataCollatorForCompletionOnlyLM(
    response_template=response_template,
    tokenizer=tokenizer,
)

trainer = SFTTrainer(
    model=model,
    train_dataset=train_dataset,
    eval_dataset=val_dataset,
    data_collator=collator,  # Masks loss on everything before closing ```
    # ... other args
)
```

```bash
# Full dataset available from Phase M
# NOTE: --max-length is data-driven from Phase M token geometry (Task M.7).
# Read the recommended value from data/traced/token_geometry.json.
# If token geometry hasn't been computed yet, use 2048 as a safe default.
python python/sft/train.py \
    --model deepseek-ai/DeepSeek-Prover-V2-7B \
    --data data/sft/train.jsonl \
    --output data/checkpoints/lora/iter_0 \
    --lora-rank 32 \
    --lora-alpha 64 \
    --lr 3e-5 \
    --epochs 3 \
    --batch-size 2 \
    --grad-accumulation 8 \
    --max-length MAX_LENGTH_FROM_TOKEN_GEOMETRY \
    --warmup-ratio 0.05 \
    --eval-steps 500 \
    --eval-data data/sft/val.jsonl \
    --completion-only       # Enable loss masking
```

**SFT training time reality check:** Batch size 2 × grad_accum 8 = effective batch 16. On ~250K pairs, one epoch = ~15.6K steps. 3 epochs = ~47K steps. At ~1.8s/step with FlashAttention, that's ~23h. **Consider reducing to 1-2 epochs** (~8-15h) for the first run, since the larger dataset compensates for fewer passes. Monitor val loss — if it plateaus after epoch 1, stop early.

**Token geometry bonus:** If Phase M Task M.7 shows p95 full-example length ≤ 1024, you can safely use `--max-length 1024` which roughly doubles throughput (halves padding waste), cutting training time significantly. See Gotcha #17.

**Using r=32 deliberately.** This matches the old pipeline exactly. Reserve r=64 for joint training (iter 1) so the comparison isolates the training method, not the LoRA capacity.

Monitor during training:
- Train/val loss curves (should converge to ~0.3–0.5 range)
- Learning rate schedule
- Gradient norms

#### Task 1.2: Merge and deploy for search (0.5h)

```bash
python python/sft/merge_lora.py \
    --base deepseek-ai/DeepSeek-Prover-V2-7B \
    --lora data/checkpoints/lora/iter_0 \
    --output data/models/merged/iter_0
```

Deploy to SGLang for inference.

### Day 4: Generate Search Trajectories

#### Task 1.3: Search on contrastive theorem pool — DEPTH OVER BREADTH (6–8h GPU)

**CRITICAL FIX:** Do NOT reduce --max-nodes to save GPU time. A choked search budget only proves shallow theorems and produces search trees that rarely reach depth 4+. The entire point of the EBM is to help at depth 4+ where the LLM's prior degrades. If you train the EBM only on shallow geometries, it learns nothing useful.

**Instead:** Search FEWER theorems with a MASSIVE budget. Depth is more valuable than breadth for EBM training. You want deep, gnarly search trees full of dead-ends and hard negative sibling collisions.

```bash
# Select 2,000 theorems that are likely to produce deep search trees.
# Filter: exclude theorems proved in < 3 tactics (they're too easy).
# Prioritize theorems from harder sources (USAMO, IMO > AMC).
python python/data/select_contrastive_pool.py \
    --input data/sft/contrastive_pool.json \
    --output data/sft/contrastive_pool_2k.json \
    --max-theorems 2000 \
    --min-depth 3 \
    --prefer-hard

# Run search with FULL budget per theorem
cargo run --release -p prover-core -- search \
    --model-path data/models/merged/iter_0 \
    --theorems data/sft/contrastive_pool_2k.json \
    --output data/trajectories/search_iter0.parquet \
    --max-nodes 800 --concurrency 10 --temperature 1.3 \
    --num-candidates 16 --batch-expansion-size 1 \
    --timeout 300
```

**Why 800 nodes × 300s:** At 16 candidates per expansion, 800 nodes gives ~50 expansions. With branching factor 16, that's enough to explore trees to depth 6–8 on the successful paths and accumulate dozens of failed branches per theorem. Each failed branch is a hard negative for the EBM.

**Expected output:**
- 2,000 theorems attempted
- ~600–1,000 proved (30–50% with full budget)
- ~200K–400K trajectory records with rich depth distribution
- Meaningful number of states at depth 4+ (target: ≥ 20% of all states)

#### Task 1.4: Quick miniF2F sanity check (1.5h GPU)

```bash
# Run on miniF2F-v2s (primary benchmark, ported to 4.27 in Phase M)
cargo run --release -p prover-core -- search \
    --model-path data/models/merged/iter_0 \
    --theorems data/benchmarks/minif2f_v2s_test_427.json \
    --output data/trajectories/minif2f_v2s_iter0.parquet \
    --max-nodes 600 --concurrency 10 --temperature 1.3 \
    --num-candidates 16 --timeout 300

# Also run v1 for backward compatibility
cargo run --release -p prover-core -- search \
    --model-path data/models/merged/iter_0 \
    --theorems data/benchmarks/minif2f_v1_test.json \
    --output data/trajectories/minif2f_v1_iter0.parquet \
    --max-nodes 600 --concurrency 10 --temperature 1.3 \
    --num-candidates 16 --timeout 300
```

This gives you the LLM-only baseline on both miniF2F-v2s and v1. Compare to old iter_4's LLM-only rate.

#### Day 4 deliverable

```
data/trajectories/
├── search_iter0.parquet          # Trajectories on competition pool
├── minif2f_v2s_iter0.parquet     # miniF2F-v2s results (primary, LLM-only)
├── minif2f_v1_iter0.parquet      # miniF2F-v1 results (backward compat)
└── trajectory_stats.json         # Proved/failed counts, depth distribution
```

---

## Phase 2: Embedding + EBM Baseline Measurement (Days 5–6)

**Goal:** Comprehensive snapshot of the embedding space and decoupled EBM quality. These numbers are the "before" in every future comparison.

---

### Day 5: Embedding Space Analysis

#### Task 2.1: Extract embeddings from iter_0 model — STATES AND GOALS (2h GPU)

For every state in the trajectory data, extract the last-hidden-state embedding. **ALSO extract the root goal embedding for each theorem** — the Day 6 GoalConditioned EBM needs both z_state and z_goal.

```python
# Load merged model
model = AutoModelForCausalLM.from_pretrained("data/models/merged/iter_0", ...)
extractor = EmbeddingExtractor(model, tokenizer)

# === State embeddings (one per trajectory record) ===
state_embeddings = []
for batch in trajectory_states:
    emb = extractor.encode(batch, max_length=512)
    state_embeddings.append(emb.cpu().float())

# === Goal embeddings (one per theorem, cached) ===
# Find the root goal state (depth==0) for each theorem
root_goals = trajectory_df[trajectory_df.depth_from_root == 0][
    ["theorem_name", "state_pp"]
].drop_duplicates(subset="theorem_name")

goal_embeddings = {}
for _, row in root_goals.iterrows():
    emb = extractor.encode([row.state_pp], max_length=512)
    goal_embeddings[row.theorem_name] = emb.cpu().float().squeeze(0)

# Save both to parquet — Day 6 EBM training needs these
save_embeddings(state_embeddings, states, "data/embeddings/iter_0/state_embeddings.parquet")
save_goal_embeddings(goal_embeddings, "data/embeddings/iter_0/goal_embeddings.parquet")

# Also join goal embedding index into the main dataframe so the
# contrastive dataloader can look up z_goal by theorem_name
trajectory_df["goal_embedding_key"] = trajectory_df["theorem_name"]
```

**Without the goal embeddings extracted here, Day 6 is blocked.** You'd have to re-run the full embedding extraction, wasting 2h of GPU time.

#### Task 2.2: Compute embedding baseline metrics (2h, no GPU)

```python
def compute_embedding_baselines(embeddings_df):
    pos = embeddings_df[embeddings_df.label == "positive"]
    neg = embeddings_df[embeddings_df.label == "negative"]
    
    metrics = {}
    
    # 1. Centroid L2
    centroid_pos = pos.embedding.mean(axis=0)
    centroid_neg = neg.embedding.mean(axis=0)
    metrics["centroid_l2"] = np.linalg.norm(centroid_pos - centroid_neg)
    
    # 2. Linear probe accuracy
    X = np.vstack([pos.embedding, neg.embedding])
    y = np.array([1]*len(pos) + [0]*len(neg))
    clf = LogisticRegression(max_iter=1000)
    metrics["linear_probe_acc"] = cross_val_score(clf, X, y, cv=5).mean()
    
    # 3. Norm gap
    metrics["norm_gap"] = pos.embedding.apply(np.linalg.norm).mean() - \
                          neg.embedding.apply(np.linalg.norm).mean()
    
    # 4. Sibling L2 distribution
    sibling_l2s = []
    for parent_id, group in embeddings_df.groupby("parent_state_id"):
        if len(group) < 2:
            continue
        embs = np.vstack(group.embedding)
        dists = pdist(embs)
        sibling_l2s.extend(dists)
    metrics["sibling_l2_mean"] = np.mean(sibling_l2s)
    metrics["sibling_l2_median"] = np.median(sibling_l2s)
    metrics["sibling_collision_rate"] = np.mean(np.array(sibling_l2s) < 1.0)
    
    # 5. Embedding variance spectrum
    all_embs = np.vstack(embeddings_df.embedding)
    cov = np.cov(all_embs.T)
    eigenvalues = np.linalg.eigvalsh(cov)[::-1]
    total_var = eigenvalues.sum()
    metrics["top1_variance_ratio"] = eigenvalues[0] / total_var
    metrics["top10_variance_ratio"] = eigenvalues[:10].sum() / total_var
    metrics["effective_dimension"] = (eigenvalues.sum())**2 / (eigenvalues**2).sum()
    
    # 6. Depth-stratified clustering
    for depth_range, label in [((0,1), "shallow"), ((2,3), "mid"), ((4,99), "deep")]:
        subset = embeddings_df[
            (embeddings_df.depth >= depth_range[0]) & 
            (embeddings_df.depth <= depth_range[1])
        ]
        if len(subset) > 100:
            labels = (subset.label == "positive").astype(int)
            score = silhouette_score(
                np.vstack(subset.embedding), labels, sample_size=min(2000, len(subset))
            )
            metrics[f"silhouette_{label}"] = score
    
    # 7. Dual-label rate
    state_labels = embeddings_df.groupby("state_pp")["label"].apply(set)
    dual = state_labels.apply(lambda s: len(s) > 1).sum()
    metrics["dual_label_states"] = dual
    metrics["dual_label_rate"] = dual / len(state_labels)
    
    return metrics
```

#### Task 2.3: Generate embedding visualizations (1h)

- t-SNE / UMAP of positive vs negative embeddings (colored by label)
- t-SNE colored by depth
- Sibling L2 histogram with collision threshold marked
- Eigenvalue spectrum plot (log scale)
- Depth vs centroid_l2 plot (does separation improve or degrade at depth?)

Save all plots to `data/evals/iter_0/plots/`.

### Day 6: EBM Baseline (PyTorch Goal-Conditioned, Decoupled)

**SCIENTIFIC INTEGRITY:** This EBM uses the SAME GoalConditionedEnergyHead architecture as iter_1 joint training. The ONLY difference between iter_0 and iter_1 EBMs will be how the backbone LoRA was trained (decoupled SFT vs joint SFT+EBM). This isolates the effect of joint training from the effect of goal conditioning.

#### Task 2.4: Train decoupled goal-conditioned EBM on frozen iter_0 embeddings (3h GPU)

```python
# Extract embeddings for all trajectory states + their theorem goals
extractor = EmbeddingExtractor(iter0_model, tokenizer)

# For each state, also encode the theorem's root goal
state_embeddings = extractor.encode(all_states, max_length=512)
goal_embeddings = extractor.encode(all_goals, max_length=512)  # one per theorem, cached

# Train GoalConditionedEnergyHead on frozen embeddings
ebm = GoalConditionedEnergyHead(d_in=4096*3, hidden_dims=[2048, 1024, 512])
optimizer = AdamW(ebm.parameters(), lr=3e-5, weight_decay=0.01)

for epoch in range(20):
    for batch in contrastive_dataloader:
        # Construct goal-conditioned input: [z_state; z_goal; z_state ⊙ z_goal]
        z_state = state_embeddings[batch.state_ids]      # frozen, no grad
        z_goal = goal_embeddings[batch.goal_ids]          # frozen, no grad
        z_interact = z_state * z_goal
        combined = torch.cat([z_state, z_goal, z_interact], dim=-1)
        
        pos_energy = ebm(combined[batch.pos_mask])
        neg_energy = ebm(combined[batch.neg_mask])
        
        loss = info_nce_loss(pos_energy, neg_energy)
        loss.backward()
        optimizer.step()
        optimizer.zero_grad()

torch.save(ebm.state_dict(), "data/checkpoints/ebm/iter_0/goal_conditioned_head.pt")
```

**This is the critical baseline.** The Day 11 comparison will be:
```
iter_0: SFT-only LoRA (r=32) + GoalConditioned EBM on FROZEN embeddings
iter_1: Joint LoRA (r=64)    + GoalConditioned EBM with LIVE gradients

Same EBM architecture. Same data. Same loss function.
Only difference: does the EBM loss flow back through the backbone?
```

#### Task 2.5: Compute EBM baseline metrics (1h)

```python
def compute_ebm_baselines(ebm_model, val_trajectories):
    metrics = {}
    
    # 1. Overall rank-1 accuracy
    # For each theorem: among all children of each parent,
    # does the EBM assign lowest energy to the one on the proof path?
    correct, total = 0, 0
    for parent_id, siblings in val_trajectories.groupby("parent_state_id"):
        if len(siblings) < 2 or siblings.label.nunique() < 2:
            continue
        energies = ebm_model.score(siblings.embedding)
        best = siblings.iloc[energies.argmin()]
        if best.label == "positive":
            correct += 1
        total += 1
    metrics["rank1_accuracy"] = correct / total
    
    # 2. Rank-1 accuracy by depth bucket
    for depth_lo, depth_hi, label in [(0,1,"d0_1"), (2,3,"d2_3"), (4,99,"d4+")]:
        subset = val_trajectories[
            (val_trajectories.depth >= depth_lo) & 
            (val_trajectories.depth <= depth_hi)
        ]
        # Same rank-1 computation on subset
        metrics[f"rank1_{label}"] = compute_rank1(ebm_model, subset)
    
    # 3. Energy gap
    pos_energy = ebm_model.score(val_trajectories[val_trajectories.label == "positive"].embedding)
    neg_energy = ebm_model.score(val_trajectories[val_trajectories.label == "negative"].embedding)
    metrics["energy_gap"] = neg_energy.mean() - pos_energy.mean()
    metrics["energy_std_pos"] = pos_energy.std()
    metrics["energy_std_neg"] = neg_energy.std()
    
    # 4. Sibling discrimination
    # Stricter than rank-1: among siblings from SAME PARENT,
    # is the proved sibling ranked first?
    metrics["sibling_discrimination"] = compute_sibling_rank1(ebm_model, val_trajectories)
    
    # 5. Hard negative active ratio
    # What fraction of margin-ranking pairs have non-zero loss?
    active = 0
    total_pairs = 0
    for pos, neg in contrastive_pairs(val_trajectories, hard_ratio=0.6):
        e_pos = ebm_model.score(pos.embedding)
        e_neg = ebm_model.score(neg.embedding)
        if e_neg - e_pos < 1.0:  # margin=1.0
            active += 1
        total_pairs += 1
    metrics["active_ratio"] = active / total_pairs
    
    return metrics
```

#### Task 2.6: EBM-augmented miniF2F search (2–3h GPU)

Run miniF2F again, now with LLM + frozen goal-conditioned EBM. The EBM scoring runs in Python (the GoalConditioned head is tiny), while tactic generation uses SGLang as before.

```bash
# Need a Python encode server that:
# 1. Loads iter_0 merged model for embedding extraction
# 2. Loads GoalConditionedEnergyHead checkpoint
# 3. Accepts (state, goal) pairs, returns energy scores
# The Rust search engine calls this server instead of burn-rs

cargo run --release -p prover-core -- search \
    --model-path data/models/merged/iter_0 \
    --ebm-server http://localhost:8081 \
    --theorems data/benchmarks/minif2f_v2s_test_427.json \
    --output data/trajectories/minif2f_v2s_iter0_ebm.parquet \
    --max-nodes 600 --concurrency 10 --temperature 1.3 \
    --num-candidates 16 --timeout 300
```

**Note:** This requires a thin Python encode server that wraps the GoalConditionedEnergyHead. Budget 1–2h to build this if the existing Rust EBM integration doesn't support external scoring servers. This server will also be needed for iter_1, so it's not wasted effort.

#### Task 2.7: Compile baseline report (1h)

File: `data/evals/iter_0/baseline_report.md`

```markdown
# v2 Iter 0 Baseline Report

## Data
- SFT pairs: ???
- Contrastive theorems: ???
- Source breakdown: Goedel 4.27 ???%, LEAN-GitHub ???%

## Embedding Space (frozen, before EBM)
| Metric                    | Value  | Old iter_4 | Notes                    |
|---------------------------|--------|------------|--------------------------|
| centroid_l2               |        | 6.40       | Higher = better          |
| linear_probe_acc          |        | 0.83       | >0.75 is healthy         |
| norm_gap                  |        | 4.29       |                          |
| sibling_collision_rate    |        | 0.14       | Lower = better           |
| top1_variance_ratio       |        | 0.645      | Lower = better spread    |
| effective_dimension       |        |            | Higher = more useful dims|
| dual_label_rate           |        |            | Lower = easier for EBM   |

## EBM (GoalConditioned PyTorch, frozen encoder, decoupled training)
| Metric                    | Value  | Old iter_5 | Notes                    |
|---------------------------|--------|------------|--------------------------|
| rank1_accuracy (overall)  |        | 0.68 train | Target: match or beat    |
| rank1_accuracy (d0-1)     |        |            |                          |
| rank1_accuracy (d2-3)     |        |            |                          |
| rank1_accuracy (d4+)      |        | 0.35-0.39  | Key metric for search    |
| energy_gap                |        |            |                          |
| sibling_discrimination    |        | 0.13       | Above 0.10 random        |
| active_ratio              |        | 0.31       | Higher = more signal     |

Note: This EBM uses GoalConditionedEnergyHead with E(state, goal),
same architecture as will be used in iter_1 joint training.
burn-rs EBM is deprecated.

## Search (miniF2F v2s test)
| Config                          | Proved | Rate   | Old iter_4 |
|---------------------------------|--------|--------|------------|
| LLM-only                        |        |        | ~35%       |
| LLM + GoalCond EBM (decoupled)  |        |        | 41.7%*     |

*Old number used context-free burn-rs EBM. Not directly comparable
due to architecture difference, but directionally informative.
```

#### Day 6 deliverable

Complete baseline snapshot. Every number in this report becomes a comparison target for joint training.

---

## Phase 3: Joint Training Loop (Days 7–8)

**Goal:** Wire the already-built GoalConditionedEnergyHead into a joint training loop with LoRA. The EBM architecture is identical to iter_0 — the only change is that gradients now flow through the backbone.

---

### Day 7: JointDataset + JointProver Model

#### Task 3.1: JointDataset for competition data (3h)

File: `python/joint/dataset.py`

(Same as before — two interleaved data streams from competition data.)

#### Task 3.2: JointProver model (2.5h)

File: `python/joint/model.py`

```python
class JointProver(nn.Module):
    def __init__(self, model_name, lora_rank=64):
        self.backbone = AutoModelForCausalLM.from_pretrained(
            model_name, torch_dtype=torch.float16,
            attn_implementation="flash_attention_2")
        
        lora_config = LoraConfig(
            r=lora_rank,          # 64 for joint training
            lora_alpha=128,
            target_modules=["q_proj","k_proj","v_proj","o_proj",
                           "gate_proj","up_proj","down_proj"],
            lora_dropout=0.05,
            task_type="CAUSAL_LM")
        self.backbone = get_peft_model(self.backbone, lora_config)
        
        self.tokenizer = AutoTokenizer.from_pretrained(model_name)
        self.tokenizer.padding_side = "right"  # Gotcha 4
        if self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token
        
        # SAME architecture as iter_0 decoupled EBM — no changes
        self.ebm_head = GoalConditionedEnergyHead(
            d_in=4096*3, hidden_dims=[2048, 1024, 512])
```

### Day 8: Training Loop + Smoke Test

#### Task 3.3: InfoNCE loss (no temperature — Gotcha 1) (0.5h)

File: `python/joint/losses.py`

```python
def info_nce_loss(pos_energy, neg_energies):
    """NO temperature parameter. EBM head handles scaling internally."""
    logits = torch.cat([-pos_energy, -neg_energies], dim=1)
    labels = torch.zeros(logits.size(0), dtype=torch.long, device=logits.device)
    return F.cross_entropy(logits, labels)
```

#### Task 3.4: Training loop with monitoring (2h)

File: `python/joint/train.py`

Full training loop with:
- Joint SFT + InfoNCE backward pass
- Gradient through pos + 1 hard neg, detach easy negs (VRAM optimization)
- Separation probe every 500 steps
- Learned temperature logging every 50 steps (Gotcha 3)
- All abort conditions from the joint training plan

#### Task 3.5: Smoke test — 1 batch forward+backward (0.5h GPU)

Verify: both losses produce finite values, gradients flow to both LoRA and EBM head, peak VRAM < 36GB.

#### Day 8 deliverable

Complete joint training infrastructure, smoke-tested on real data.

---

## Phase 4: Joint Training + Evaluation (Days 9–11)

**Goal:** Train iter_1 with joint training, evaluate against iter_0 baselines.

---

### Day 9: Joint Training Run

#### Task 4.1: Train iter_1 (6–8h GPU)

```bash
python python/joint/train.py \
    --model deepseek-ai/DeepSeek-Prover-V2-7B \
    --sft-data data/sft/train.jsonl \
    --trajectories data/trajectories/search_iter0.parquet \
    --output data/checkpoints/lora/iter_1/ \
    --lora-rank 64 \
    --lr-lora 2e-5 \
    --lr-ebm 3e-5 \
    --lambda-ebm 0.1 \
    --total-steps 6000 \
    --warmup-steps 300 \
    --k-negatives 3 \
    --hard-ratio 0.6 \
    --eval-every 500 \
    --save-every 1000
```

**ABORT CONDITIONS:**
```
centroid_l2 < 3.0 at any eval      → increase λ to 0.3, restart from checkpoint
SFT loss > 0.5 after step 2000     → decrease λ to 0.03, restart
EBM rank-1 < 0.25 after step 2000  → check data pipeline
Temperature hits 0.1 or 10.0 floor/ceiling for 500+ steps → debug energy scale
Any NaN                             → check gradient clipping, temperature clamp
```

**Expected monitoring trajectory:**
```
Step     SFT loss   EBM loss   centroid_l2   temperature   λ·EBM
0        ~0.45      ~2.08      baseline      ~1.0          0.21
500      ~0.35      ~1.5       ≥ baseline    0.5–3.0       0.15
1000     ~0.32      ~1.2       ≥ baseline    0.5–3.0       0.12
3000     ~0.29      ~0.9       ≥ baseline+1  0.5–3.0       0.09
6000     ~0.28      ~0.8       ≥ baseline+1  0.5–3.0       0.08
```

### Day 10: Evaluation

#### Task 4.2: Export and deploy (0.5h)

```python
merged = model.backbone.merge_and_unload()
merged.save_pretrained("data/models/merged/iter_1")
torch.save(model.ebm_head.state_dict(), "data/checkpoints/ebm/iter_1/goal_conditioned_head.pt")
```

#### Task 4.3: Repeat ALL baseline measurements on iter_1 (4h GPU)

Extract embeddings, compute every metric from Phase 2:
- Centroid L2, linear probe, sibling L2, variance spectrum, etc.
- **DO NOT** train a separate frozen-encoder EBM. Instead, compare THREE configurations:

```
Config A: iter_0 LoRA + GoalCond EBM on frozen iter_0 embeddings    (Phase 2 baseline)
Config B: iter_1 LoRA + GoalCond EBM on frozen iter_1 embeddings    (joint LoRA helps embeddings?)
Config C: iter_1 LoRA + iter_1's jointly-trained GoalCond EBM        (full joint system)
```

Config B is the key control. It uses the SAME decoupled EBM training recipe as iter_0, but on embeddings from the jointly-trained LoRA. If B > A, joint training improved the embedding space itself (the primary thesis). If C > B, the live-gradient EBM head is additionally better than frozen-embedding training.

This gives a clean 3-row comparison table:

```
                           Embedding quality         EBM quality            End-to-end
Config                   centroid_l2  probe_acc    rank1_d4+  sibling    miniF2F
──────────────────────────────────────────────────────────────────────────────────
A: iter_0 decoupled         ???        ???          ???        ???        ???
B: iter_1 LoRA, decoupled   ???        ???          ???        ???        ???  
C: iter_1 full joint        ???        ???          ???        ???        ???
```

- A→B difference = effect of joint LoRA training on embedding quality
- B→C difference = effect of live EBM gradients on EBM head quality
- A→C difference = total system improvement (what you'd report)

This decomposition is the scientific payoff of building the GoalConditioned head on Day 1.

### Day 11: Analysis + Decision

#### Task 4.4: Comparative analysis (2h)

File: `data/evals/iter_1/analysis/comparison_report.md`

Key questions to answer:

1. **Did competition data help?** Compare iter_0 miniF2F (LLM-only) vs old iter_4 miniF2F (LLM-only). If iter_0 is significantly better despite less total data, the distribution shift thesis is confirmed.

2. **Did joint training protect embeddings?** Compare iter_1 centroid_l2 vs iter_0 centroid_l2. If iter_1 ≥ iter_0, joint training prevented the collapse that destroyed iter_5. This is the primary success criterion.

3. **Did goal conditioning help the EBM?** Compare iter_1 goal-conditioned EBM rank-1 vs iter_1 frozen EBM rank-1 vs iter_0 frozen EBM rank-1. If goal conditioning doesn't help, the dual-label problem may not be as severe in competition data (which has fewer abstract theorems with reusable lemmas).

4. **End-to-end: did miniF2F improve?** Compare iter_1 best config vs iter_0 best config vs old 41.7%. This is the number that matters.

#### Task 4.5: Decision point

```
IF miniF2F (iter_1) > miniF2F (iter_0) AND centroid_l2 held:
    → Joint training works. Plan iter_2: generate new trajectories
      with iter_1 model, retrain (expert iteration + joint training).
    
IF miniF2F (iter_0) >> old 41.7% BUT iter_1 ≈ iter_0:
    → Data was the bottleneck, not training method. Consider whether
      joint training overhead is justified, or just do more expert
      iteration with competition data.
    
IF miniF2F (iter_0) ≈ old 41.7%:
    → Data distribution wasn't the problem. Need to investigate
      search strategy, model capacity, or inference budget.
    
IF centroid_l2 collapsed in iter_1:
    → Joint training failed. Try alternating steps (even=SFT, odd=EBM)
      or increase λ significantly.
```

---

## Summary Timeline

```
Phase M (prerequisite, ~1 week):
Day M.0:  Clone Goedel, update toolchain to 4.27, bulk compile       (0h GPU, CPU bound)
Day M.1:  Automated fixes (renames, instance patches), rebuild       (0h GPU, CPU bound)
Day M.2:  Manual triage, target ≥95% survival                       (0h GPU)
Day M.3:  Port miniF2F-v2s/v2c to 4.27, verify 488 compile          (0h GPU)
Day M.4:  LeanDojo trace on compiled Goedel 4.27 proofs              (0h GPU, CPU bound ~10-15h)
Day M.5:  Data exploration (integrity, tokens, depths, tactics)      (0h GPU)
          + Download + filter LEAN-GitHub, merge into unified SFT
Day M.6:  Release Goedel-4.27 on HuggingFace, community contribution(0h GPU)

Phase 0-4 (after Phase M):
Day  1:  Validate Phase M outputs, archive old iterations            (0h GPU)
Day  2:  Data quality checks, format verification                    (0h GPU)
Day  3:  PyTorch GoalCond EBM + SFT training on full data            (8-10h GPU)
Day  4:  Generate deep trajectories (2K theorems × 800 nodes)        (8h GPU)
         + miniF2F v2s + v1 LLM-only baselines
Day  5:  Extract embeddings, compute embedding baselines              (2h GPU)
Day  6:  Train decoupled GoalCond EBM, EBM metrics, EBM+miniF2F      (5h GPU)
         + build Python encode server for search integration
Day  7:  JointDataset, JointProver model                              (0h GPU)
Day  8:  Training loop, smoke test                                    (1h GPU)
Day  9:  Joint training run (iter_1, 6K steps)                        (8h GPU)
Day 10:  Full evaluation: 3-config comparison (A, B, C)               (6h GPU)
Day 11:  Analysis, decomposed attribution report, decision            (0h GPU)
────────────────────────────────────────────────────────────────────────────────
Phase 0-4 GPU:  ~38h  →  ~$40-55 at A100 $1.29/hr
Phase 0-4 wall clock: 10-12 days (can compress to 8–9 with overnight jobs)
Phase M: ~1 week (CPU-bound, minimal cost)
```

## Directory Structure

```
burn-qed/
├── data/                                # ALL generated/downloaded artifacts (gitignored)
│   ├── lean/                            # Raw HF dataset downloads
│   │   ├── workbook/goedel_proofs_427/  # Phase M: Goedel proofs migrated to Lean 4.27
│   │   ├── lean_github/                 # Phase M: Raw LEAN-GitHub download
│   │   └── numinamath/                  # Raw download (Phase 2, deferred)
│   │
│   ├── benchmarks/                      # Evaluation benchmark JSONs (tracked in git)
│   │   ├── minif2f_v2s_test_427.json    # Phase M: miniF2F-v2s ported to 4.27
│   │   ├── minif2f_v2c_test_427.json    # Phase M: miniF2F-v2c ported to 4.27
│   │   └── minif2f_v1_test.json         # miniF2F v1 (backward compat)
│   │
│   ├── traced/                          # LeanDojo tracing output (Phase M)
│   │   ├── goedel_427_pairs.jsonl       # ~110-140K tactic pairs from Goedel 4.27
│   │   ├── lean_github_pairs.jsonl      # ~100-150K pairs from LEAN-GitHub
│   │   ├── token_geometry.json          # Data-driven max_length for SFT & EmbeddingExtractor
│   │   ├── exploration_report.json      # Depths, tactic vocab, imports, clean pool
│   │   └── exploration_plots/           # Histograms (token lengths, depth dist, tactic freq)
│   │
│   ├── sft/                             # Formatted SFT training data
│   │   ├── train.jsonl                  # Merged: ~210-350K pairs
│   │   ├── val.jsonl                    # 5% held out, theorem-level split
│   │   ├── contrastive_pool.json        # Goedel theorems with depth ≥ 3
│   │   └── stats.json                   # Counts, source breakdown
│   │
│   ├── models/                          # Model weights
│   │   ├── base/deepseek-prover-v2-7b/  # Downloaded HF base model
│   │   └── merged/                      # LoRA-merged models for SGLang
│   │       ├── iter_0/                  # SFT-only LoRA r=32 merged
│   │       └── iter_1/                  # Joint LoRA r=64 merged
│   │
│   ├── checkpoints/                     # Training checkpoints
│   │   ├── lora/                        # LoRA adapter weights
│   │   │   ├── iter_0/                  # SFT-only r=32
│   │   │   └── iter_1/                  # Joint r=64
│   │   └── ebm/                         # GoalConditioned EBM heads
│   │       ├── iter_0/goal_conditioned_head.pt  # DECOUPLED
│   │       └── iter_1/goal_conditioned_head.pt  # JOINT
│   │
│   ├── trajectories/                    # Search result parquets
│   │   ├── search_iter0.parquet         # Trajectories on competition pool
│   │   ├── minif2f_v2s_iter0.parquet    # miniF2F-v2s (LLM-only)
│   │   └── minif2f_v1_iter0.parquet     # miniF2F-v1 (backward compat)
│   │
│   ├── embeddings/                      # Extracted embeddings
│   │   ├── iter_0/                      # Frozen embeddings (states + goals)
│   │   └── iter_1/
│   │
│   ├── evals/                           # Evaluation results & reports
│   │   ├── iter_0/                      # Baseline metrics + plots
│   │   │   ├── embedding_metrics.json
│   │   │   ├── ebm_metrics.json
│   │   │   ├── search_results.json
│   │   │   ├── baseline_report.md
│   │   │   └── plots/
│   │   └── iter_1/                      # Comparison metrics
│   │       ├── config_B_frozen_ebm/     # Decoupled EBM on iter_1 embeddings
│   │       └── analysis/
│   │           └── comparison_report.md  # 3-config A/B/C decomposition
│   │
│   └── archive/v1/                      # Archived v1 artifacts
│
├── python/
│   ├── encode_server.py             # Embedding extraction server (nf4)
│   ├── migration/                   # Phase M migration scripts
│   │   ├── auto_fix.py              # Automated Mathlib rename fixes
│   │   ├── parallel_trace.py        # Parallelized LeanDojo tracing
│   │   ├── port_minif2f_v2.py       # Port miniF2F-v2 statements to 4.27
│   │   ├── filter_lean_github.py    # LEAN-GitHub quality filter
│   │   └── explore_traced.py        # Data exploration (integrity, tokens, structure)
│   ├── training/                    # LLM fine-tuning scripts (SFT)
│   └── joint/                       # v2 joint training
│       ├── __init__.py
│       ├── ebm_head.py
│       ├── dataset.py
│       ├── losses.py
│       ├── model.py
│       ├── monitoring.py
│       └── train.py
│
├── crates/                          # Rust core (search, lean-repl, policy, trajectory, prover-core)
│   └── ebm/                         # burn-rs EBM (v1, behind --features burn-ebm)
├── scripts/                         # Server startup, eval orchestration
├── configs/                         # TOML configs
├── docs/                            # Documentation
└── vendor/                          # Git submodules (Pantograph)
```

## Key Hyperparameters

| Parameter         | iter_0 (baseline) | iter_1 (joint)   | Rationale                              |
|-------------------|--------------------|-------------------|----------------------------------------|
| LoRA rank         | 32                 | 64                | Isolate capacity effect                |
| LoRA alpha        | 64                 | 128               | Standard 2× rank                       |
| lr_lora           | 3e-5               | 2e-5              | Slightly lower for joint stability     |
| lr_ebm            | —                  | 3e-5              | Same as decoupled EBM training         |
| λ_ebm             | —                  | 0.1               | Start conservative                     |
| SFT data          | competition only   | competition only  | Same data, different method            |
| Contrastive data  | —                  | iter_0 trajectories| Generated by baseline model           |
| K negatives       | —                  | 3                 | VRAM limited                           |
| hard_ratio        | 0.6 (EBM)         | 0.6 (EBM)        | Consistent with decoupled              |
| total_steps       | ~1-2 epochs (~15-31K steps, ~8-15h) | 6000 (~5h)            | Larger dataset; monitor val loss for early stopping |
| SFT max_length    | data-driven (Task M.7) | data-driven (Task M.7) | Set by token geometry; default 2048 if unknown |
| Encode max_length | data-driven (Task M.7) | data-driven (Task M.7) | Set by token geometry; default 512 if unknown  |

## Success Metrics

| Metric                       | iter_0 target | iter_1 target | Red line   |
|------------------------------|---------------|---------------|------------|
| miniF2F v2s (LLM-only)       | ≥ 35%         | ≥ 35%         | < 25%      |
| miniF2F v2s (LLM+EBM)        | ≥ 40%         | ≥ 45%         | < 35%      |
| centroid_l2                   | ≥ 5.0         | ≥ iter_0 val  | < 3.0      |
| linear_probe_acc              | ≥ 0.75        | ≥ iter_0 val  | < 0.65     |
| EBM rank-1 (d4+)             | ≥ 0.30        | ≥ 0.40        | < 0.25     |
| sibling_discrimination        | ≥ 0.12        | ≥ 0.15        | < 0.10     |
| SFT val loss                  | ≤ 0.50        | ≤ 0.50        | > 0.65     |

## Gotchas (Carried Forward + New)

1. **Temperature Double-Dip:** InfoNCE loss has NO temperature parameter. EBM head handles it.
2. **25M Parameter Init:** First GoalConditionedEnergyHead layer: `weight.data *= 0.1`
3. **Monitor Temperature:** Log every 50 steps. Healthy range [0.5, 3.0]. Floor/ceiling = ABORT.
4. **Tokenizer Padding:** `padding_side = "right"` for encode(). Always. Verify the last-token indexing math: `seq_lengths = attention_mask.sum(dim=1) - 1` grabs the last CONTENT token only if padding is on the right. Check that this isn't grabbing an `<eos>` token that was appended after your actual content — you want the last semantic token, not a special token.
5. **Lean Version:** Resolved by Phase M. Goedel migrated to 4.27 before tracing; LEAN-GitHub pre-traced strings are version-agnostic for SFT. NuminaMath (v4.15) deferred to Phase 2. miniF2F-v2s/v2c ported to 4.27.
6. **LeanDojo Tracing Time:** Budget 10–15h wall clock for 30K theorems, not 4–6h. Parallelize across all CPU cores. This happens in Phase M (Task M.5). Cap workers: `NUM_WORKERS = min(16, int(cpu_count() * 0.75))` (see Gotcha 15).
7. **Confounding Variable:** iter_0 and iter_1 MUST use identical EBM architecture (GoalConditionedEnergyHead). If you compare context-free E(state) against goal-conditioned E(state, goal), you cannot attribute improvements to joint training. The burn-rs EBM pipeline is DEPRECATED as of this plan.
8. **Search Depth for EBM Training:** Shallow search trees (max_nodes=300) produce shallow trajectories that starve the EBM of depth-4+ training signal. Use fewer theorems with larger budgets (800 nodes, 300s timeout) to get deep, gnarly search trees.
9. **Goal Embeddings in Phase 2:** Task 2.1 must extract BOTH state embeddings AND root goal embeddings. The Day 6 GoalConditioned EBM requires z_goal for every training sample. If you only extract z_state, you'll have to re-run the entire 2h embedding extraction.
10. **Zombie Lean Processes:** LeanDojo tracing (Phase M, Task M.5) spawns headless Lean REPLs as child processes. If a Python worker crashes or times out, the Lean process survives as a zombie consuming RAM. After 4h of parallel tracing, dozens of zombies will crash the instance. Call `cleanup_lean_processes()` at every chunk boundary and add per-theorem `signal.alarm()` timeouts. See `parallel_trace.py` in Phase M.
11. **Validation Split Leak:** Split SFT train/val by THEOREM NAME, not by individual tactic pairs. Random pair-level splitting leaks tactics from the same theorem into both sets, inflating val accuracy and masking overfitting. The model memorizes variable names (x, y, h1) and specific proof context.
12. **Sorry/Admit Filter:** Community datasets (Lean Workbook, GitHub scrapes) contain proofs that compile but use `sorry`, `admit`, or `cheat` to bypass hard steps. Filter these during tracing, not after — reject the entire theorem if any tactic contains a banned keyword. This was diagnosed in v1 (2 miniF2F "proofs" used sorry) and must not be forgotten.
13. **SFT Loss Masking (Capacity Bleed):** Proof states are 500+ tokens, tactics are 10–50 tokens. Using `DataCollatorForCompletionOnlyLM` with `response_template="` ``` `\n"` (closing code fence) masks loss on everything before the tactic. Without this, 90% of LoRA gradient updates are wasted learning to autoregress the environment state. The model will *appear* to converge (low SFT loss) while learning nothing useful. This is the #1 silent performance killer for LoRA-based theorem provers. See `docs/data_format_spec.md`.
14. **Prompt Format:** Uses DeepSeek-native format with tactic state as Lean comment inside code fence. No special tokens (`[GOAL]`/`[PROOFSTEP]` are NOT used) — eliminates tokenizer fragmentation risk entirely. The model's existing vocabulary handles Lean comments and code fences natively. See `docs/data_format_spec.md`.
15. **CPU Worker OOM:** LeanDojo tracing (Phase M) is memory-intensive per process. Spinning up 30 simultaneous headless Lean REPLs will trigger the Linux OOM-killer within 10 minutes, silently killing workers without Python exceptions. Cap: `NUM_WORKERS = min(16, int(cpu_count() * 0.75))`.
16. **Phase M Survival Rate:** If Goedel 4.27 migration survival drops below 90% (< 26,783 of 29,759), the SFT dataset may be too small even with LEAN-GitHub supplement. Investigate: are failures clustered in specific Mathlib areas? Can automated rename scripts recover more? Below 80%, consider staying on an older toolchain or using LeanInteract for multi-version validation.
17. **Token Geometry Truncation:** Do NOT assume `--max-length 2048` for SFT training. Phase M Task M.7 computes the actual token length distribution. If p95 full-example length is ≤ 1024, using 2048 wastes ~50% VRAM on padding — doubling max_length roughly halves throughput. Conversely, if competition proofs have longer states than Mathlib (possible for multi-step number theory), 2048 may silently truncate training signal. Always read `data/traced/token_geometry.json` before setting `--max-length`.