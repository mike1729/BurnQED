# burn-qed v2: Competition-Focused Execution Plan

**Strategic shift:** Drop Mathlib4 as primary training source. Retrain from scratch on competition-level data (Lean Workbook, NuminaMath-LEAN) that matches miniF2F evaluation distribution. Reset iteration counter. Establish rigorous embedding and EBM baselines before introducing joint training.

**Why:** LoRA r=32–64 has limited capacity. Spending it on Mathlib's 122K abstract theorems (category theory, topology, measure theory) when the eval is competition algebra/number theory is wasting parameters on out-of-distribution patterns. Workbook proofs were generated by DeepSeek-Prover-V1.5 — already in our base model's natural tactic vocabulary.

**Hardware:** Lambda Labs A100 40GB, ~$1.29/hr.

**Timeline:** 10–12 days, ~$40–55 GPU budget.

---

## Phase 0: Environment Setup + Data Pipeline (Days 1–3)

**Goal:** Port GoalConditionedEnergyHead to PyTorch FIRST (eliminates confounding variable on Day 11), verify Lean version compatibility, trace competition datasets into tactic pairs, archive old iterations.

**CRITICAL DESIGN DECISION:** The PyTorch GoalConditionedEnergyHead must be built on Day 1, NOT Day 7. Both the iter_0 decoupled baseline AND the iter_1 joint run must use the identical EBM architecture. If iter_0 uses burn-rs context-free E(state) and iter_1 uses PyTorch goal-conditioned E(state, goal), the Day 11 comparison is scientifically meaningless — you cannot attribute improvements to joint training vs goal conditioning. The burn-rs EBM pipeline is now deprecated. All EBM work going forward is PyTorch only.

---

### Day 1: PyTorch EBM Port + Lean Version Audit

#### Task 0.0: Port GoalConditionedEnergyHead to PyTorch (3h)

File: `v2/python/joint/ebm_head.py`

This is now the FIRST task in the entire plan. Both iter_0 and iter_1 use this exact architecture.

```python
class GoalConditionedEnergyHead(nn.Module):
    """
    E(state, goal) — used for BOTH decoupled (iter_0) and joint (iter_1).
    Input: [z_state; z_goal; z_state ⊙ z_goal] → 12288
    Architecture: 12288 → 2048 → 1024 → 512 → 1
    Spectral norm on all layers. SiLU activations. Dropout 0.15.
    Learnable log_temperature clamped [0.1, 10.0].
    First layer: weight *= 0.1 (Gotcha 2).
    """
```

Also implement a thin encode() wrapper for extracting embeddings from the base model:

```python
class EmbeddingExtractor:
    """Standalone embedding extraction (no LoRA dependency).
    Used in both decoupled training (frozen encoder) and joint training.
    """
    def __init__(self, model, tokenizer):
        self.model = model
        self.tokenizer = tokenizer
        self.tokenizer.padding_side = "right"  # Gotcha 4
    
    def encode(self, texts, max_length=512):
        inputs = self.tokenizer(texts, return_tensors="pt",
                                padding=True, truncation=True, 
                                max_length=max_length)
        outputs = self.model(**inputs, output_hidden_states=True)
        hidden = outputs.hidden_states[-1]
        seq_lengths = inputs["attention_mask"].sum(dim=1) - 1
        return hidden[torch.arange(hidden.size(0)), seq_lengths]
```

Unit tests:
- GoalConditionedEnergyHead: random input → output shape [batch, 1], spectral norms ≈ 1.0
- Verify first layer weight magnitude is ~0.1x other layers after init
- Verify temperature parameter exists and forward() divides by exp(log_temp)
- Verify EmbeddingExtractor grabs correct token (not pad) with padding_side="right"

#### Task 0.1: Lean version audit (2h)

This is the highest-risk task. If versions are incompatible, the entire data plan changes.

```
Your LeanDojo setup:     Lean ??? / Mathlib ???
Lean Workbook:           Lean v4.8.0-rc1 / Mathlib v4.8.0-rc1
NuminaMath-LEAN:         Lean ??? (check HF card)
Goedel-Prover proofs:    Lean v4.8.0-rc1 (same as Workbook)
```

Steps:
1. Check your current LeanDojo Lean/Mathlib version: `lean --version` in your project
2. Download Lean Workbook repo, inspect `lean-toolchain` and `lakefile.lean`
3. Download NuminaMath-LEAN, check which Lean version its statements compile against
4. If mismatch: set up a separate conda env with the correct Lean version for tracing

**Decision point:**
```
IF versions match within one minor version:
    → Trace directly, minor fixups expected
IF major mismatch (e.g., v4.3 vs v4.8):
    → Set up parallel Lean installation for tracing only
    → Budget an extra half-day
IF Workbook proofs simply won't trace:
    → Fall back to NuminaMath-LEAN only
    → Or use Goedel-Prover-V2's data which may be on a newer Lean
```

#### Task 0.2: Archive old iterations (1h)

```bash
mkdir -p archive/mathlib_iterations
mv models/llm/iter_* archive/mathlib_iterations/
mv checkpoints/ebm/iter_* archive/mathlib_iterations/
mv trajectories/*.parquet archive/mathlib_iterations/
mv docs/iter_* archive/mathlib_iterations/
mv docs/ebm_perf.md archive/mathlib_iterations/

# Keep the search engine, SGLang configs, and base model
# Create v2 directory structure
mkdir -p v2/{iter_0/{model,tactic_pairs,trajectories,embeddings,ebm,baselines},data/{workbook,numinamath,traced}}
```

#### Task 0.3: Download datasets (1h, can run in parallel)

```bash
# Goedel Workbook proofs (29.7K proved theorems)
huggingface-cli download Goedel-LM/Lean-workbook-proofs --local-dir v2/data/workbook/goedel_proofs

# Lean Workbook full (57K + 83K statements, many with proofs)
huggingface-cli download internlm/Lean-Workbook --local-dir v2/data/workbook/full

# NuminaMath-LEAN (100K competition statements, subset with proofs)
huggingface-cli download AI-MO/NuminaMath-LEAN --local-dir v2/data/numinamath
```

Inventory after download:
```
Dataset                    Statements    With proofs (est.)    Source distribution
─────────────────────────────────────────────────────────────────────────────────
Lean Workbook              57,231        ~15K (prior work)     Math contest forums
Lean Workbook Plus         82,893        ~10K (est.)           Extended contests
Goedel Workbook proofs     29,700        29,700                Goedel-Prover solved
NuminaMath-LEAN            100,000       ~20K (est.)           IMO/USAMO/AMC/AIME
─────────────────────────────────────────────────────────────────────────────────
Total available proofs:                  ~55–75K (before tracing losses)
```

### Day 2–3: Tracing + Tactic Pair Extraction

**REALITY CHECK:** LeanDojo tracing is NOT text parsing. It spins up a headless Lean REPL per theorem, compiles the full environment, and steps through every tactic to extract local context. On a single core, one theorem takes 5–30 seconds. 30K theorems × 15s average = 125 hours single-threaded. This WILL blow your timeline if you don't parallelize.

#### Task 0.4: Trace Goedel Workbook proofs — PARALLEL + CHUNKED (Day 2: 8–12h wall clock)

**Strategy:** Trace a 20% seed batch (6K proofs) first, format it, and start SFT training on Day 3. Let the remaining 80% trace overnight/in background. Do NOT wait for full completion.

```python
#!/usr/bin/env python3
"""parallel_trace.py — Parallelized LeanDojo tracing with chunked output."""

import json
import signal
import multiprocessing as mp
from concurrent.futures import ProcessPoolExecutor, as_completed
from pathlib import Path
from lean_dojo import LeanGitRepo, trace
import psutil

NUM_WORKERS = min(16, int(mp.cpu_count() * 0.75))  # Cap: 30 Lean REPLs will OOM 200GB RAM
CHUNK_SIZE = 500              # Write output every 500 theorems
SEED_FRACTION = 0.20          # Trace this fraction first, then start training
PER_THEOREM_TIMEOUT = 120     # Kill any theorem that takes > 2 minutes

# === SORRY/ADMIT FILTER (carried from v1 diagnosis) ===
BANNED_TACTICS = ["sorry", "admit", "cheat"]

def cleanup_lean_processes():
    """Kill orphaned Lean child processes that survive worker crashes.
    LeanDojo spawns headless Lean REPLs as subprocesses. If a Python worker
    dies mid-trace, the Lean process becomes a zombie consuming RAM.
    Call this at every chunk boundary and on any exception."""
    killed = 0
    for proc in psutil.process_iter(['name', 'pid']):
        try:
            if proc.info['name'] and 'lean' in proc.info['name'].lower():
                proc.kill()
                killed += 1
        except (psutil.NoSuchProcess, psutil.AccessDenied):
            pass
    if killed > 0:
        print(f"  [cleanup] Killed {killed} orphaned Lean processes")

def trace_single_theorem(theorem_data):
    """Trace one theorem. Returns list of tactic pairs or error string.
    Includes: per-theorem timeout, sorry/admit filter, zombie prevention."""
    try:
        # Set alarm for per-theorem timeout (Unix only)
        def timeout_handler(signum, frame):
            raise TimeoutError(f"Tracing exceeded {PER_THEOREM_TIMEOUT}s")
        signal.signal(signal.SIGALRM, timeout_handler)
        signal.alarm(PER_THEOREM_TIMEOUT)
        
        traced = trace(theorem_data)
        signal.alarm(0)  # Cancel alarm on success
        
        pairs = []
        for state, tactic in traced.tactic_pairs:
            # === sorry/admit/cheat filter ===
            if any(bad in tactic.text for bad in BANNED_TACTICS):
                return {"status": "error", "error": "Contains sorry/admit/cheat",
                        "name": theorem_data["name"]}
            
            pairs.append({
                "state": state.pp,
                "tactic": tactic.text,
                "theorem": theorem_data["name"],
                "depth": state.depth,
                "source": theorem_data.get("source", "workbook"),
                "num_goals": len(state.goals),
            })
        return {"status": "ok", "pairs": pairs, "name": theorem_data["name"]}
    except Exception as e:
        signal.alarm(0)  # Cancel alarm on error
        return {"status": "error", "error": str(e), "name": theorem_data["name"]}

def trace_batch(theorems, output_dir, batch_label):
    """Trace a batch of theorems with all available CPU cores."""
    results = []
    errors = []
    all_pairs = []
    
    with ProcessPoolExecutor(max_workers=NUM_WORKERS) as executor:
        futures = {executor.submit(trace_single_theorem, t): t 
                   for t in theorems}
        
        for i, future in enumerate(as_completed(futures)):
            try:
                result = future.result(timeout=PER_THEOREM_TIMEOUT + 30)
            except Exception as e:
                result = {"status": "error", "error": str(e),
                          "name": futures[future].get("name", "unknown")}
            
            if result["status"] == "ok":
                all_pairs.extend(result["pairs"])
                results.append(result["name"])
            else:
                errors.append(result)
            
            # Write checkpoint + kill zombies every CHUNK_SIZE completions
            if (i + 1) % CHUNK_SIZE == 0:
                print(f"[{batch_label}] {i+1}/{len(theorems)} done, "
                      f"{len(results)} ok, {len(errors)} errors, "
                      f"{len(all_pairs)} pairs so far")
                flush_pairs(all_pairs, output_dir / f"pairs_partial_{i+1}.jsonl")
                cleanup_lean_processes()  # Kill zombie Lean processes
    
    # Final cleanup
    cleanup_lean_processes()
    return all_pairs, errors

if __name__ == "__main__":
    theorems = load_all_theorems()  # 30K from Goedel Workbook
    
    # Shuffle and split: 20% seed, 80% remainder
    random.shuffle(theorems)
    split = int(len(theorems) * SEED_FRACTION)
    seed_batch = theorems[:split]
    remainder = theorems[split:]
    
    # Phase 1: Trace seed batch (should take ~2-3h with full parallelism)
    print(f"Tracing seed batch: {len(seed_batch)} theorems on {NUM_WORKERS} cores")
    seed_pairs, seed_errors = trace_batch(seed_batch, output_dir, "SEED")
    save_jsonl(seed_pairs, "v2/data/traced/sft_train_seed.jsonl")
    print(f"Seed complete: {len(seed_pairs)} pairs, {len(seed_errors)} errors "
          f"({len(seed_errors)/len(seed_batch)*100:.1f}% failure rate)")
    
    # >>> At this point, Day 3 SFT training can START on seed data <<<
    
    # Phase 2: Trace remainder (runs overnight / in background)
    print(f"Tracing remainder: {len(remainder)} theorems")
    rem_pairs, rem_errors = trace_batch(remainder, output_dir, "REMAINDER")
    
    # Combine
    all_pairs = seed_pairs + rem_pairs
    save_jsonl(all_pairs, "v2/data/traced/sft_train_full.jsonl")
```

**Expected timing with parallelization:**
```
Machine: Lambda A100 instance (typically 32–64 CPU cores)
Seed batch (6K theorems): ~2–3h wall clock
Remainder (24K theorems): ~8–12h wall clock (runs in background)
Total: ~10–15h, but training starts after just 2–3h
```

**Critical: check error rate after seed batch.** If > 20% failures, STOP and debug Lean version mismatch before wasting time on the remainder.

#### Task 0.5: Trace NuminaMath-LEAN proved subset (runs in parallel with Task 0.4 remainder)

Same parallelized pipeline. Filter to `formal_proof != ""` first. Run on a separate tmux pane while the Workbook remainder traces.

#### Task 0.6: Filter and format tactic pairs (1h, after seed batch completes)

```python
# Combine all traced pairs
all_pairs = workbook_pairs + numinamath_pairs

# === CRITICAL: Split by THEOREM, not by tactic pair ===
# If you do a random split on pairs, tactics from the same theorem
# leak into both train and val. The LLM memorizes variable names
# (x, y, h1) and val loss looks artificially good.
unique_theorems = list(set(p['theorem'] for p in all_pairs))
random.shuffle(unique_theorems)
split_idx = int(len(unique_theorems) * 0.95)
train_theorems = set(unique_theorems[:split_idx])

sft_train = [p for p in all_pairs if p['theorem'] in train_theorems]
sft_val = [p for p in all_pairs if p['theorem'] not in train_theorems]

# Format for SFT training (same format as previous iterations)
for pair in sft_train:
    formatted = f"[GOAL]{pair['state']}[PROOFSTEP]{pair['tactic']}"
    sft_data.append(formatted)

# Separate pool for contrastive training (need depth ≥ 3)
for theorem in all_theorems:
    if theorem.max_depth >= 3:
        contrastive_pool.append(theorem)

# Stats to report
print(f"Total SFT pairs: {len(sft_train)} train, {len(sft_val)} val")
print(f"Theorems: {len(train_theorems)} train, "
      f"{len(unique_theorems) - len(train_theorems)} val")
print(f"Theorems with depth ≥ 3 (contrastive pool): {len(contrastive_pool)}")
print(f"Sorry/admit filtered: check tracing_report.json")
```

**Depth filter rationale:** Proofs with depth < 3 (single `omega`, `norm_num`, `simp` closers) have no meaningful search tree. They're fine for teaching the LLM tactics but useless for EBM training because there's nothing to discriminate.

#### Day 2–3 deliverable (progressive)

After seed batch (Day 2 evening):
```
v2/data/traced/
├── sft_train_seed.jsonl       # ~15K–30K pairs from 6K theorems (enough to start SFT)
└── tracing_report_seed.json   # Error rate — ABORT if > 20%
```

After full tracing (Day 3, runs in background during SFT):
```
v2/data/traced/
├── sft_train_full.jsonl       # All tactic pairs (target: ≥ 80K)
├── sft_val.jsonl              # 5% held out, stratified by source
├── contrastive_theorems.json  # Theorems with depth ≥ 3
├── tracing_report.json        # Full success/failure rates
└── stats.json                 # Counts, depth distribution, source breakdown
```

---

## Phase 1: Iter 0 SFT Baseline (Days 3–4)

**Goal:** Train LoRA on competition data using the existing pipeline. Start on seed data (20% of traced proofs), retrain on full data once tracing completes. Measure everything about the embedding space before touching the EBM.

**Note:** Remaining 80% of proofs are still tracing in background from Day 2. SFT training begins on the seed batch.

---

### Day 3: SFT Training

#### Task 1.1: Train iter_0 LoRA on seed data first, full data when ready (5–7h GPU)

**CRITICAL: Completion-only loss masking.** Proof states are 500+ tokens. Tactics are 10–50 tokens. Without masking, 90% of gradient updates teach the LoRA to echo the environment state — a complete waste of its limited r=32 capacity. The model must ONLY compute loss on the tactic tokens after `[PROOFSTEP]`.

```python
from trl import SFTTrainer, DataCollatorForCompletionOnlyLM

# === Special token check (do this BEFORE training) ===
# DeepSeek-Prover-V1.5 may not have [GOAL] and [PROOFSTEP] as native tokens.
# If the tokenizer shatters them into subwords (['[', 'GO', 'AL', ']']),
# the model loses structural boundaries between state and tactic.
special_tokens = ['[GOAL]', '[PROOFSTEP]']
tokens_to_add = [t for t in special_tokens 
                 if t not in tokenizer.get_vocab()]

if tokens_to_add:
    special_tokens_dict = {'additional_special_tokens': tokens_to_add}
    num_added = tokenizer.add_special_tokens(special_tokens_dict)
    model.resize_token_embeddings(len(tokenizer))
    print(f"Added {num_added} special tokens, resized embeddings to {len(tokenizer)}")
    # IMPORTANT: If using LoRA, ensure embed_tokens and lm_head are in
    # modules_to_save so the new token embeddings are actually trainable:
    # lora_config = LoraConfig(..., modules_to_save=["embed_tokens", "lm_head"])

# === Completion-only collator — loss only on tactic tokens ===
response_template = "[PROOFSTEP]"
collator = DataCollatorForCompletionOnlyLM(
    response_template=response_template,
    tokenizer=tokenizer,
)

trainer = SFTTrainer(
    model=model,
    train_dataset=train_dataset,
    eval_dataset=val_dataset,
    data_collator=collator,  # Masks loss on everything before [PROOFSTEP]
    # ... other args
)
```

```bash
# Start with seed data (available immediately from Day 2)
python python/sft/train.py \
    --model deepseek-ai/DeepSeek-Prover-V1.5-Base \
    --data v2/data/traced/sft_train_seed.jsonl \
    --output v2/iter_0/model_seed \
    --lora-rank 32 \
    --lora-alpha 64 \
    --lr 3e-5 \
    --epochs 3 \
    --batch-size 2 \
    --grad-accumulation 8 \
    --max-length 2048 \
    --warmup-ratio 0.05 \
    --eval-steps 500 \
    --eval-data v2/data/traced/sft_val.jsonl \
    --completion-only       # Enable loss masking

# Once full tracing completes (check background job):
# Retrain from scratch on full dataset — this is the real iter_0
python python/sft/train.py \
    --model deepseek-ai/DeepSeek-Prover-V1.5-Base \
    --data v2/data/traced/sft_train_full.jsonl \
    --output v2/iter_0/model \
    --lora-rank 32 \
    ... (same hyperparams, same --completion-only)
```

**SFT training time reality check:** Batch size 2 × grad_accum 8 = effective batch 16. On 80K pairs, one epoch = 5K steps. 3 epochs = 15K steps. At ~1.8s/step with FlashAttention, that's ~7.5h, not 4–5h. Budget accordingly. The seed-data run (~15K pairs) will be ~3 epochs × 940 steps ≈ 30 min — fast enough to validate the pipeline before committing to the full run.

**Using r=32 deliberately.** This matches the old pipeline exactly. Reserve r=64 for joint training (iter 1) so the comparison isolates the training method, not the LoRA capacity.

Monitor during training:
- Train/val loss curves (should converge to ~0.3–0.5 range)
- Learning rate schedule
- Gradient norms

#### Task 1.2: Merge and deploy for search (0.5h)

```bash
python python/sft/merge_lora.py \
    --base deepseek-ai/DeepSeek-Prover-V1.5-Base \
    --lora v2/iter_0/model \
    --output v2/iter_0/model/merged
```

Deploy to SGLang for inference.

### Day 4: Generate Search Trajectories

#### Task 1.3: Search on contrastive theorem pool — DEPTH OVER BREADTH (6–8h GPU)

**CRITICAL FIX:** Do NOT reduce --max-nodes to save GPU time. A choked search budget only proves shallow theorems and produces search trees that rarely reach depth 4+. The entire point of the EBM is to help at depth 4+ where the LLM's prior degrades. If you train the EBM only on shallow geometries, it learns nothing useful.

**Instead:** Search FEWER theorems with a MASSIVE budget. Depth is more valuable than breadth for EBM training. You want deep, gnarly search trees full of dead-ends and hard negative sibling collisions.

```bash
# Select 2,000 theorems that are likely to produce deep search trees.
# Filter: exclude theorems proved in < 3 tactics (they're too easy).
# Prioritize theorems from harder sources (USAMO, IMO > AMC).
python v2/python/data/select_contrastive_pool.py \
    --input v2/data/traced/contrastive_theorems.json \
    --output v2/data/traced/contrastive_pool_2k.json \
    --max-theorems 2000 \
    --min-depth 3 \
    --prefer-hard

# Run search with FULL budget per theorem
cargo run --release -p prover-core -- search \
    --model-path v2/iter_0/model/merged \
    --theorems v2/data/traced/contrastive_pool_2k.json \
    --output v2/iter_0/trajectories/search_iter0.parquet \
    --max-nodes 800 --concurrency 10 --temperature 1.3 \
    --num-candidates 16 --batch-expansion-size 1 \
    --timeout 300
```

**Why 800 nodes × 300s:** At 16 candidates per expansion, 800 nodes gives ~50 expansions. With branching factor 16, that's enough to explore trees to depth 6–8 on the successful paths and accumulate dozens of failed branches per theorem. Each failed branch is a hard negative for the EBM.

**Expected output:**
- 2,000 theorems attempted
- ~600–1,000 proved (30–50% with full budget)
- ~200K–400K trajectory records with rich depth distribution
- Meaningful number of states at depth 4+ (target: ≥ 20% of all states)

#### Task 1.4: Quick miniF2F sanity check (1.5h GPU)

```bash
cargo run --release -p prover-core -- search \
    --model-path v2/iter_0/model/merged \
    --theorems data/minif2f_v2s_test.json \
    --output v2/iter_0/trajectories/minif2f_iter0.parquet \
    --max-nodes 600 --concurrency 10 --temperature 1.3 \
    --num-candidates 16 --timeout 300
```

This gives you the LLM-only baseline on miniF2F with competition-focused training. Compare to old iter_4's LLM-only rate.

#### Day 4 deliverable

```
v2/iter_0/trajectories/
├── search_iter0.parquet     # Trajectories on competition pool
├── minif2f_iter0.parquet    # miniF2F results (LLM-only baseline)
└── trajectory_stats.json    # Proved/failed counts, depth distribution
```

---

## Phase 2: Embedding + EBM Baseline Measurement (Days 5–6)

**Goal:** Comprehensive snapshot of the embedding space and decoupled EBM quality. These numbers are the "before" in every future comparison.

---

### Day 5: Embedding Space Analysis

#### Task 2.1: Extract embeddings from iter_0 model — STATES AND GOALS (2h GPU)

For every state in the trajectory data, extract the last-hidden-state embedding. **ALSO extract the root goal embedding for each theorem** — the Day 6 GoalConditioned EBM needs both z_state and z_goal.

```python
# Load merged model
model = AutoModelForCausalLM.from_pretrained("v2/iter_0/model/merged", ...)
extractor = EmbeddingExtractor(model, tokenizer)

# === State embeddings (one per trajectory record) ===
state_embeddings = []
for batch in trajectory_states:
    emb = extractor.encode(batch, max_length=512)
    state_embeddings.append(emb.cpu().float())

# === Goal embeddings (one per theorem, cached) ===
# Find the root goal state (depth==0) for each theorem
root_goals = trajectory_df[trajectory_df.depth_from_root == 0][
    ["theorem_name", "state_pp"]
].drop_duplicates(subset="theorem_name")

goal_embeddings = {}
for _, row in root_goals.iterrows():
    emb = extractor.encode([row.state_pp], max_length=512)
    goal_embeddings[row.theorem_name] = emb.cpu().float().squeeze(0)

# Save both to parquet — Day 6 EBM training needs these
save_embeddings(state_embeddings, states, "v2/iter_0/embeddings/state_embeddings.parquet")
save_goal_embeddings(goal_embeddings, "v2/iter_0/embeddings/goal_embeddings.parquet")

# Also join goal embedding index into the main dataframe so the
# contrastive dataloader can look up z_goal by theorem_name
trajectory_df["goal_embedding_key"] = trajectory_df["theorem_name"]
```

**Without the goal embeddings extracted here, Day 6 is blocked.** You'd have to re-run the full embedding extraction, wasting 2h of GPU time.

#### Task 2.2: Compute embedding baseline metrics (2h, no GPU)

```python
def compute_embedding_baselines(embeddings_df):
    pos = embeddings_df[embeddings_df.label == "positive"]
    neg = embeddings_df[embeddings_df.label == "negative"]
    
    metrics = {}
    
    # 1. Centroid L2
    centroid_pos = pos.embedding.mean(axis=0)
    centroid_neg = neg.embedding.mean(axis=0)
    metrics["centroid_l2"] = np.linalg.norm(centroid_pos - centroid_neg)
    
    # 2. Linear probe accuracy
    X = np.vstack([pos.embedding, neg.embedding])
    y = np.array([1]*len(pos) + [0]*len(neg))
    clf = LogisticRegression(max_iter=1000)
    metrics["linear_probe_acc"] = cross_val_score(clf, X, y, cv=5).mean()
    
    # 3. Norm gap
    metrics["norm_gap"] = pos.embedding.apply(np.linalg.norm).mean() - \
                          neg.embedding.apply(np.linalg.norm).mean()
    
    # 4. Sibling L2 distribution
    sibling_l2s = []
    for parent_id, group in embeddings_df.groupby("parent_state_id"):
        if len(group) < 2:
            continue
        embs = np.vstack(group.embedding)
        dists = pdist(embs)
        sibling_l2s.extend(dists)
    metrics["sibling_l2_mean"] = np.mean(sibling_l2s)
    metrics["sibling_l2_median"] = np.median(sibling_l2s)
    metrics["sibling_collision_rate"] = np.mean(np.array(sibling_l2s) < 1.0)
    
    # 5. Embedding variance spectrum
    all_embs = np.vstack(embeddings_df.embedding)
    cov = np.cov(all_embs.T)
    eigenvalues = np.linalg.eigvalsh(cov)[::-1]
    total_var = eigenvalues.sum()
    metrics["top1_variance_ratio"] = eigenvalues[0] / total_var
    metrics["top10_variance_ratio"] = eigenvalues[:10].sum() / total_var
    metrics["effective_dimension"] = (eigenvalues.sum())**2 / (eigenvalues**2).sum()
    
    # 6. Depth-stratified clustering
    for depth_range, label in [((0,1), "shallow"), ((2,3), "mid"), ((4,99), "deep")]:
        subset = embeddings_df[
            (embeddings_df.depth >= depth_range[0]) & 
            (embeddings_df.depth <= depth_range[1])
        ]
        if len(subset) > 100:
            labels = (subset.label == "positive").astype(int)
            score = silhouette_score(
                np.vstack(subset.embedding), labels, sample_size=min(2000, len(subset))
            )
            metrics[f"silhouette_{label}"] = score
    
    # 7. Dual-label rate
    state_labels = embeddings_df.groupby("state_pp")["label"].apply(set)
    dual = state_labels.apply(lambda s: len(s) > 1).sum()
    metrics["dual_label_states"] = dual
    metrics["dual_label_rate"] = dual / len(state_labels)
    
    return metrics
```

#### Task 2.3: Generate embedding visualizations (1h)

- t-SNE / UMAP of positive vs negative embeddings (colored by label)
- t-SNE colored by depth
- Sibling L2 histogram with collision threshold marked
- Eigenvalue spectrum plot (log scale)
- Depth vs centroid_l2 plot (does separation improve or degrade at depth?)

Save all plots to `v2/iter_0/baselines/plots/`.

### Day 6: EBM Baseline (PyTorch Goal-Conditioned, Decoupled)

**SCIENTIFIC INTEGRITY:** This EBM uses the SAME GoalConditionedEnergyHead architecture as iter_1 joint training. The ONLY difference between iter_0 and iter_1 EBMs will be how the backbone LoRA was trained (decoupled SFT vs joint SFT+EBM). This isolates the effect of joint training from the effect of goal conditioning.

#### Task 2.4: Train decoupled goal-conditioned EBM on frozen iter_0 embeddings (3h GPU)

```python
# Extract embeddings for all trajectory states + their theorem goals
extractor = EmbeddingExtractor(iter0_model, tokenizer)

# For each state, also encode the theorem's root goal
state_embeddings = extractor.encode(all_states, max_length=512)
goal_embeddings = extractor.encode(all_goals, max_length=512)  # one per theorem, cached

# Train GoalConditionedEnergyHead on frozen embeddings
ebm = GoalConditionedEnergyHead(d_in=4096*3, hidden_dims=[2048, 1024, 512])
optimizer = AdamW(ebm.parameters(), lr=3e-5, weight_decay=0.01)

for epoch in range(20):
    for batch in contrastive_dataloader:
        # Construct goal-conditioned input: [z_state; z_goal; z_state ⊙ z_goal]
        z_state = state_embeddings[batch.state_ids]      # frozen, no grad
        z_goal = goal_embeddings[batch.goal_ids]          # frozen, no grad
        z_interact = z_state * z_goal
        combined = torch.cat([z_state, z_goal, z_interact], dim=-1)
        
        pos_energy = ebm(combined[batch.pos_mask])
        neg_energy = ebm(combined[batch.neg_mask])
        
        loss = info_nce_loss(pos_energy, neg_energy)
        loss.backward()
        optimizer.step()
        optimizer.zero_grad()

torch.save(ebm.state_dict(), "v2/iter_0/ebm/goal_conditioned_head.pt")
```

**This is the critical baseline.** The Day 11 comparison will be:
```
iter_0: SFT-only LoRA (r=32) + GoalConditioned EBM on FROZEN embeddings
iter_1: Joint LoRA (r=64)    + GoalConditioned EBM with LIVE gradients

Same EBM architecture. Same data. Same loss function.
Only difference: does the EBM loss flow back through the backbone?
```

#### Task 2.5: Compute EBM baseline metrics (1h)

```python
def compute_ebm_baselines(ebm_model, val_trajectories):
    metrics = {}
    
    # 1. Overall rank-1 accuracy
    # For each theorem: among all children of each parent,
    # does the EBM assign lowest energy to the one on the proof path?
    correct, total = 0, 0
    for parent_id, siblings in val_trajectories.groupby("parent_state_id"):
        if len(siblings) < 2 or siblings.label.nunique() < 2:
            continue
        energies = ebm_model.score(siblings.embedding)
        best = siblings.iloc[energies.argmin()]
        if best.label == "positive":
            correct += 1
        total += 1
    metrics["rank1_accuracy"] = correct / total
    
    # 2. Rank-1 accuracy by depth bucket
    for depth_lo, depth_hi, label in [(0,1,"d0_1"), (2,3,"d2_3"), (4,99,"d4+")]:
        subset = val_trajectories[
            (val_trajectories.depth >= depth_lo) & 
            (val_trajectories.depth <= depth_hi)
        ]
        # Same rank-1 computation on subset
        metrics[f"rank1_{label}"] = compute_rank1(ebm_model, subset)
    
    # 3. Energy gap
    pos_energy = ebm_model.score(val_trajectories[val_trajectories.label == "positive"].embedding)
    neg_energy = ebm_model.score(val_trajectories[val_trajectories.label == "negative"].embedding)
    metrics["energy_gap"] = neg_energy.mean() - pos_energy.mean()
    metrics["energy_std_pos"] = pos_energy.std()
    metrics["energy_std_neg"] = neg_energy.std()
    
    # 4. Sibling discrimination
    # Stricter than rank-1: among siblings from SAME PARENT,
    # is the proved sibling ranked first?
    metrics["sibling_discrimination"] = compute_sibling_rank1(ebm_model, val_trajectories)
    
    # 5. Hard negative active ratio
    # What fraction of margin-ranking pairs have non-zero loss?
    active = 0
    total_pairs = 0
    for pos, neg in contrastive_pairs(val_trajectories, hard_ratio=0.6):
        e_pos = ebm_model.score(pos.embedding)
        e_neg = ebm_model.score(neg.embedding)
        if e_neg - e_pos < 1.0:  # margin=1.0
            active += 1
        total_pairs += 1
    metrics["active_ratio"] = active / total_pairs
    
    return metrics
```

#### Task 2.6: EBM-augmented miniF2F search (2–3h GPU)

Run miniF2F again, now with LLM + frozen goal-conditioned EBM. The EBM scoring runs in Python (the GoalConditioned head is tiny), while tactic generation uses SGLang as before.

```bash
# Need a Python encode server that:
# 1. Loads iter_0 merged model for embedding extraction
# 2. Loads GoalConditionedEnergyHead checkpoint
# 3. Accepts (state, goal) pairs, returns energy scores
# The Rust search engine calls this server instead of burn-rs

cargo run --release -p prover-core -- search \
    --model-path v2/iter_0/model/merged \
    --ebm-server http://localhost:8081 \
    --theorems data/minif2f_v2s_test.json \
    --output v2/iter_0/trajectories/minif2f_iter0_ebm.parquet \
    --max-nodes 600 --concurrency 10 --temperature 1.3 \
    --num-candidates 16 --timeout 300
```

**Note:** This requires a thin Python encode server that wraps the GoalConditionedEnergyHead. Budget 1–2h to build this if the existing Rust EBM integration doesn't support external scoring servers. This server will also be needed for iter_1, so it's not wasted effort.

#### Task 2.7: Compile baseline report (1h)

File: `v2/iter_0/baselines/baseline_report.md`

```markdown
# v2 Iter 0 Baseline Report

## Data
- SFT pairs: ???
- Contrastive theorems: ???
- Source breakdown: Workbook ???%, NuminaMath ???%

## Embedding Space (frozen, before EBM)
| Metric                    | Value  | Old iter_4 | Notes                    |
|---------------------------|--------|------------|--------------------------|
| centroid_l2               |        | 6.40       | Higher = better          |
| linear_probe_acc          |        | 0.83       | >0.75 is healthy         |
| norm_gap                  |        | 4.29       |                          |
| sibling_collision_rate    |        | 0.14       | Lower = better           |
| top1_variance_ratio       |        | 0.645      | Lower = better spread    |
| effective_dimension       |        |            | Higher = more useful dims|
| dual_label_rate           |        |            | Lower = easier for EBM   |

## EBM (GoalConditioned PyTorch, frozen encoder, decoupled training)
| Metric                    | Value  | Old iter_5 | Notes                    |
|---------------------------|--------|------------|--------------------------|
| rank1_accuracy (overall)  |        | 0.68 train | Target: match or beat    |
| rank1_accuracy (d0-1)     |        |            |                          |
| rank1_accuracy (d2-3)     |        |            |                          |
| rank1_accuracy (d4+)      |        | 0.35-0.39  | Key metric for search    |
| energy_gap                |        |            |                          |
| sibling_discrimination    |        | 0.13       | Above 0.10 random        |
| active_ratio              |        | 0.31       | Higher = more signal     |

Note: This EBM uses GoalConditionedEnergyHead with E(state, goal),
same architecture as will be used in iter_1 joint training.
burn-rs EBM is deprecated.

## Search (miniF2F v2s test)
| Config                          | Proved | Rate   | Old iter_4 |
|---------------------------------|--------|--------|------------|
| LLM-only                        |        |        | ~35%       |
| LLM + GoalCond EBM (decoupled)  |        |        | 41.7%*     |

*Old number used context-free burn-rs EBM. Not directly comparable
due to architecture difference, but directionally informative.
```

#### Day 6 deliverable

Complete baseline snapshot. Every number in this report becomes a comparison target for joint training.

---

## Phase 3: Joint Training Loop (Days 7–8)

**Goal:** Wire the already-built GoalConditionedEnergyHead into a joint training loop with LoRA. The EBM architecture is identical to iter_0 — the only change is that gradients now flow through the backbone.

---

### Day 7: JointDataset + JointProver Model

#### Task 3.1: JointDataset for competition data (3h)

File: `v2/python/joint/dataset.py`

(Same as before — two interleaved data streams from competition data.)

#### Task 3.2: JointProver model (2.5h)

File: `v2/python/joint/model.py`

```python
class JointProver(nn.Module):
    def __init__(self, model_name, lora_rank=64):
        self.backbone = AutoModelForCausalLM.from_pretrained(
            model_name, torch_dtype=torch.float16,
            attn_implementation="flash_attention_2")
        
        lora_config = LoraConfig(
            r=lora_rank,          # 64 for joint training
            lora_alpha=128,
            target_modules=["q_proj","k_proj","v_proj","o_proj",
                           "gate_proj","up_proj","down_proj"],
            lora_dropout=0.05,
            task_type="CAUSAL_LM")
        self.backbone = get_peft_model(self.backbone, lora_config)
        
        self.tokenizer = AutoTokenizer.from_pretrained(model_name)
        self.tokenizer.padding_side = "right"  # Gotcha 4
        if self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token
        
        # SAME architecture as iter_0 decoupled EBM — no changes
        self.ebm_head = GoalConditionedEnergyHead(
            d_in=4096*3, hidden_dims=[2048, 1024, 512])
```

### Day 8: Training Loop + Smoke Test

#### Task 3.3: InfoNCE loss (no temperature — Gotcha 1) (0.5h)

File: `v2/python/joint/losses.py`

```python
def info_nce_loss(pos_energy, neg_energies):
    """NO temperature parameter. EBM head handles scaling internally."""
    logits = torch.cat([-pos_energy, -neg_energies], dim=1)
    labels = torch.zeros(logits.size(0), dtype=torch.long, device=logits.device)
    return F.cross_entropy(logits, labels)
```

#### Task 3.4: Training loop with monitoring (2h)

File: `v2/python/joint/train.py`

Full training loop with:
- Joint SFT + InfoNCE backward pass
- Gradient through pos + 1 hard neg, detach easy negs (VRAM optimization)
- Separation probe every 500 steps
- Learned temperature logging every 50 steps (Gotcha 3)
- All abort conditions from the joint training plan

#### Task 3.5: Smoke test — 1 batch forward+backward (0.5h GPU)

Verify: both losses produce finite values, gradients flow to both LoRA and EBM head, peak VRAM < 36GB.

#### Day 8 deliverable

Complete joint training infrastructure, smoke-tested on real data.

---

## Phase 4: Joint Training + Evaluation (Days 9–11)

**Goal:** Train iter_1 with joint training, evaluate against iter_0 baselines.

---

### Day 9: Joint Training Run

#### Task 4.1: Train iter_1 (6–8h GPU)

```bash
python v2/python/joint/train.py \
    --model deepseek-ai/DeepSeek-Prover-V1.5-Base \
    --sft-data v2/data/traced/sft_train.jsonl \
    --trajectories v2/iter_0/trajectories/search_iter0.parquet \
    --output v2/iter_1/ \
    --lora-rank 64 \
    --lr-lora 2e-5 \
    --lr-ebm 3e-5 \
    --lambda-ebm 0.1 \
    --total-steps 6000 \
    --warmup-steps 300 \
    --k-negatives 3 \
    --hard-ratio 0.6 \
    --eval-every 500 \
    --save-every 1000
```

**ABORT CONDITIONS:**
```
centroid_l2 < 3.0 at any eval      → increase λ to 0.3, restart from checkpoint
SFT loss > 0.5 after step 2000     → decrease λ to 0.03, restart
EBM rank-1 < 0.25 after step 2000  → check data pipeline
Temperature hits 0.1 or 10.0 floor/ceiling for 500+ steps → debug energy scale
Any NaN                             → check gradient clipping, temperature clamp
```

**Expected monitoring trajectory:**
```
Step     SFT loss   EBM loss   centroid_l2   temperature   λ·EBM
0        ~0.45      ~2.08      baseline      ~1.0          0.21
500      ~0.35      ~1.5       ≥ baseline    0.5–3.0       0.15
1000     ~0.32      ~1.2       ≥ baseline    0.5–3.0       0.12
3000     ~0.29      ~0.9       ≥ baseline+1  0.5–3.0       0.09
6000     ~0.28      ~0.8       ≥ baseline+1  0.5–3.0       0.08
```

### Day 10: Evaluation

#### Task 4.2: Export and deploy (0.5h)

```python
merged = model.backbone.merge_and_unload()
merged.save_pretrained("v2/iter_1/model/merged")
torch.save(model.ebm_head.state_dict(), "v2/iter_1/ebm/goal_conditioned_head.pt")
```

#### Task 4.3: Repeat ALL baseline measurements on iter_1 (4h GPU)

Extract embeddings, compute every metric from Phase 2:
- Centroid L2, linear probe, sibling L2, variance spectrum, etc.
- **DO NOT** train a separate frozen-encoder EBM. Instead, compare THREE configurations:

```
Config A: iter_0 LoRA + GoalCond EBM on frozen iter_0 embeddings    (Phase 2 baseline)
Config B: iter_1 LoRA + GoalCond EBM on frozen iter_1 embeddings    (joint LoRA helps embeddings?)
Config C: iter_1 LoRA + iter_1's jointly-trained GoalCond EBM        (full joint system)
```

Config B is the key control. It uses the SAME decoupled EBM training recipe as iter_0, but on embeddings from the jointly-trained LoRA. If B > A, joint training improved the embedding space itself (the primary thesis). If C > B, the live-gradient EBM head is additionally better than frozen-embedding training.

This gives a clean 3-row comparison table:

```
                           Embedding quality         EBM quality            End-to-end
Config                   centroid_l2  probe_acc    rank1_d4+  sibling    miniF2F
──────────────────────────────────────────────────────────────────────────────────
A: iter_0 decoupled         ???        ???          ???        ???        ???
B: iter_1 LoRA, decoupled   ???        ???          ???        ???        ???  
C: iter_1 full joint        ???        ???          ???        ???        ???
```

- A→B difference = effect of joint LoRA training on embedding quality
- B→C difference = effect of live EBM gradients on EBM head quality
- A→C difference = total system improvement (what you'd report)

This decomposition is the scientific payoff of building the GoalConditioned head on Day 1.

### Day 11: Analysis + Decision

#### Task 4.4: Comparative analysis (2h)

File: `v2/iter_1/analysis/comparison_report.md`

Key questions to answer:

1. **Did competition data help?** Compare iter_0 miniF2F (LLM-only) vs old iter_4 miniF2F (LLM-only). If iter_0 is significantly better despite less total data, the distribution shift thesis is confirmed.

2. **Did joint training protect embeddings?** Compare iter_1 centroid_l2 vs iter_0 centroid_l2. If iter_1 ≥ iter_0, joint training prevented the collapse that destroyed iter_5. This is the primary success criterion.

3. **Did goal conditioning help the EBM?** Compare iter_1 goal-conditioned EBM rank-1 vs iter_1 frozen EBM rank-1 vs iter_0 frozen EBM rank-1. If goal conditioning doesn't help, the dual-label problem may not be as severe in competition data (which has fewer abstract theorems with reusable lemmas).

4. **End-to-end: did miniF2F improve?** Compare iter_1 best config vs iter_0 best config vs old 41.7%. This is the number that matters.

#### Task 4.5: Decision point

```
IF miniF2F (iter_1) > miniF2F (iter_0) AND centroid_l2 held:
    → Joint training works. Plan iter_2: generate new trajectories
      with iter_1 model, retrain (expert iteration + joint training).
    
IF miniF2F (iter_0) >> old 41.7% BUT iter_1 ≈ iter_0:
    → Data was the bottleneck, not training method. Consider whether
      joint training overhead is justified, or just do more expert
      iteration with competition data.
    
IF miniF2F (iter_0) ≈ old 41.7%:
    → Data distribution wasn't the problem. Need to investigate
      search strategy, model capacity, or inference budget.
    
IF centroid_l2 collapsed in iter_1:
    → Joint training failed. Try alternating steps (even=SFT, odd=EBM)
      or increase λ significantly.
```

---

## Summary Timeline

```
Day  1:  PyTorch GoalCond EBM, Lean version audit, archive, download  (0h GPU)
Day  2:  Parallel tracing — seed batch (20%) ready by evening         (0h GPU, CPU bound)
Day  3:  SFT on seed → retrain on full when ready (background trace)  (7.5h GPU)
Day  4:  Generate deep trajectories (2K theorems × 800 nodes)         (8h GPU)
         + miniF2F LLM-only baseline
Day  5:  Extract embeddings, compute embedding baselines              (2h GPU)
Day  6:  Train decoupled GoalCond EBM, EBM metrics, EBM+miniF2F      (5h GPU)
         + build Python encode server for search integration
Day  7:  JointDataset, JointProver model                              (0h GPU)
Day  8:  Training loop, smoke test                                    (1h GPU)
Day  9:  Joint training run (iter_1, 6K steps)                        (8h GPU)
Day 10:  Full evaluation: 3-config comparison (A, B, C)               (6h GPU)
Day 11:  Analysis, decomposed attribution report, decision            (0h GPU)
────────────────────────────────────────────────────────────────────────────────
Total GPU:  ~38h  →  ~$49 at A100 $1.29/hr
Total wall clock: 11 days (can compress to 8–9 with overnight jobs)
```

## Directory Structure

```
burn-qed/
├── archive/
│   └── mathlib_iterations/          # Everything from old iter_0–5
│       ├── models/
│       ├── checkpoints/
│       ├── trajectories/
│       └── docs/
│
├── v2/
│   ├── data/
│   │   ├── workbook/                # Raw downloads
│   │   ├── numinamath/              # Raw downloads
│   │   └── traced/                  # Processed tactic pairs
│   │       ├── sft_train.jsonl
│   │       ├── sft_val.jsonl
│   │       └── contrastive_theorems.json
│   │
│   ├── iter_0/                      # SFT-only baseline
│   │   ├── model/                   # LoRA r=32 + merged
│   │   ├── tactic_pairs/            # Symlink to traced/
│   │   ├── trajectories/            # Search results
│   │   ├── embeddings/              # Frozen embeddings (states + goals)
│   │   ├── ebm/                     # GoalConditioned PyTorch head (DECOUPLED)
│   │   │   └── goal_conditioned_head.pt
│   │   └── baselines/               # All metrics + plots
│   │       ├── embedding_metrics.json
│   │       ├── ebm_metrics.json
│   │       ├── search_results.json
│   │       ├── baseline_report.md
│   │       └── plots/
│   │
│   ├── iter_1/                      # Joint training
│   │   ├── model/                   # LoRA r=64 + merged
│   │   ├── ebm/                     # GoalConditioned PyTorch head (JOINT)
│   │   │   └── goal_conditioned_head.pt
│   │   ├── trajectories/
│   │   ├── embeddings/
│   │   ├── baselines/               # Same metrics as iter_0
│   │   │   └── config_B_frozen_ebm/ # Decoupled EBM retrained on iter_1 embeddings
│   │   └── analysis/
│   │       └── comparison_report.md  # 3-config A/B/C decomposition
│   │
│   └── python/
│       └── joint/
│           ├── __init__.py
│           ├── ebm_head.py
│           ├── dataset.py
│           ├── losses.py
│           ├── model.py
│           ├── monitoring.py
│           ├── train.py
│           └── export.py
│
├── crates/                          # Existing Rust search engine (unchanged)
├── python/                          # Existing SFT scripts (unchanged)
└── docs/
    ├── v2_execution_plan.md         # This document
    └── joint_training_plan.md       # Architecture reference (keep)
```

## Key Hyperparameters

| Parameter         | iter_0 (baseline) | iter_1 (joint)   | Rationale                              |
|-------------------|--------------------|-------------------|----------------------------------------|
| LoRA rank         | 32                 | 64                | Isolate capacity effect                |
| LoRA alpha        | 64                 | 128               | Standard 2× rank                       |
| lr_lora           | 3e-5               | 2e-5              | Slightly lower for joint stability     |
| lr_ebm            | —                  | 3e-5              | Same as decoupled EBM training         |
| λ_ebm             | —                  | 0.1               | Start conservative                     |
| SFT data          | competition only   | competition only  | Same data, different method            |
| Contrastive data  | —                  | iter_0 trajectories| Generated by baseline model           |
| K negatives       | —                  | 3                 | VRAM limited                           |
| hard_ratio        | 0.6 (EBM)         | 0.6 (EBM)        | Consistent with decoupled              |
| total_steps       | ~3 epochs (~15K steps, ~7.5h) | 6000 (~5h)            |                                        |
| SFT max_length    | 2048               | 2048              |                                        |
| Encode max_length | —                  | 512               | Proof states are shorter               |

## Success Metrics

| Metric                       | iter_0 target | iter_1 target | Red line   |
|------------------------------|---------------|---------------|------------|
| miniF2F v2s (LLM-only)       | ≥ 35%         | ≥ 35%         | < 25%      |
| miniF2F v2s (LLM+EBM)        | ≥ 40%         | ≥ 45%         | < 35%      |
| centroid_l2                   | ≥ 5.0         | ≥ iter_0 val  | < 3.0      |
| linear_probe_acc              | ≥ 0.75        | ≥ iter_0 val  | < 0.65     |
| EBM rank-1 (d4+)             | ≥ 0.30        | ≥ 0.40        | < 0.25     |
| sibling_discrimination        | ≥ 0.12        | ≥ 0.15        | < 0.10     |
| SFT val loss                  | ≤ 0.50        | ≤ 0.50        | > 0.65     |

## Gotchas (Carried Forward + New)

1. **Temperature Double-Dip:** InfoNCE loss has NO temperature parameter. EBM head handles it.
2. **25M Parameter Init:** First GoalConditionedEnergyHead layer: `weight.data *= 0.1`
3. **Monitor Temperature:** Log every 50 steps. Healthy range [0.5, 3.0]. Floor/ceiling = ABORT.
4. **Tokenizer Padding:** `padding_side = "right"` for encode(). Always. Verify the last-token indexing math: `seq_lengths = attention_mask.sum(dim=1) - 1` grabs the last CONTENT token only if padding is on the right. Check that this isn't grabbing an `<eos>` token that was appended after your actual content — you want the last semantic token, not a special token.
5. **Lean Version:** Check FIRST before committing to any dataset. Mismatch = lost day.
6. **LeanDojo Tracing Time:** Budget 10–15h wall clock for 30K theorems, not 4–6h. Parallelize across all CPU cores. Trace a 20% seed batch first and start training immediately — don't block on full completion.
7. **Confounding Variable:** iter_0 and iter_1 MUST use identical EBM architecture (GoalConditionedEnergyHead). If you compare context-free E(state) against goal-conditioned E(state, goal), you cannot attribute improvements to joint training. The burn-rs EBM pipeline is DEPRECATED as of this plan.
8. **Search Depth for EBM Training:** Shallow search trees (max_nodes=300) produce shallow trajectories that starve the EBM of depth-4+ training signal. Use fewer theorems with larger budgets (800 nodes, 300s timeout) to get deep, gnarly search trees.
9. **Goal Embeddings in Phase 2:** Task 2.1 must extract BOTH state embeddings AND root goal embeddings. The Day 6 GoalConditioned EBM requires z_goal for every training sample. If you only extract z_state, you'll have to re-run the entire 2h embedding extraction.
10. **Zombie Lean Processes:** LeanDojo tracing spawns headless Lean REPLs as child processes. If a Python worker crashes or times out, the Lean process survives as a zombie consuming RAM. After 4h of parallel tracing, dozens of zombies will crash the Lambda instance. Call `cleanup_lean_processes()` at every chunk boundary and add per-theorem `signal.alarm()` timeouts.
11. **Validation Split Leak:** Split SFT train/val by THEOREM NAME, not by individual tactic pairs. Random pair-level splitting leaks tactics from the same theorem into both sets, inflating val accuracy and masking overfitting. The model memorizes variable names (x, y, h1) and specific proof context.
12. **Sorry/Admit Filter:** Community datasets (Lean Workbook, GitHub scrapes) contain proofs that compile but use `sorry`, `admit`, or `cheat` to bypass hard steps. Filter these during tracing, not after — reject the entire theorem if any tactic contains a banned keyword. This was diagnosed in v1 (2 miniF2F "proofs" used sorry) and must not be forgotten.
13. **SFT Loss Masking (Capacity Bleed):** Proof states are 500+ tokens, tactics are 10–50 tokens. Without a `DataCollatorForCompletionOnlyLM` masking loss on everything before `[PROOFSTEP]`, 90% of LoRA gradient updates are wasted learning to autoregress the environment state. The model will *appear* to converge (low SFT loss) while learning nothing useful. This is the #1 silent performance killer for LoRA-based theorem provers.
14. **Special Token Fragmentation:** `[GOAL]` and `[PROOFSTEP]` may not be native tokens in DeepSeek-Prover-V1.5's vocabulary. If fragmented into sub-words (`['[', 'GO', 'AL', ']']`), the model loses its structural boundary between reading and generating. Check `tokenizer.get_vocab()` before training. If missing, `add_special_tokens()` + `resize_token_embeddings()` + `modules_to_save=["embed_tokens", "lm_head"]` in LoRA config.
15. **CPU Worker OOM:** A Lambda 1×A100 has ~30 vCPUs and ~200GB system RAM. LeanDojo tracing is memory-intensive per process. Spinning up 30 simultaneous headless Lean REPLs will trigger the Linux OOM-killer within 10 minutes, silently killing workers without Python exceptions. Cap: `NUM_WORKERS = min(16, int(cpu_count() * 0.75))`.
